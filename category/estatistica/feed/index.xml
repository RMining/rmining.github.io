<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Estatística &#8211; R Mining</title>
	<atom:link href="./index.html" rel="self" type="application/rss+xml" />
	<link>./../../../index.html</link>
	<description>Mineração de Dados, Estatística, Tecnologia</description>
	<lastBuildDate>Tue, 17 Jan 2017 10:04:59 +0000</lastBuildDate>
	<language>pt-BR</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.7.1</generator>
	<item>
		<title>Onde estudar Estatística?</title>
		<link>./../../../2016/04/21/onde-estudar-estatistica/index.html</link>
		<comments>./../../../2016/04/21/onde-estudar-estatistica/index.html#respond</comments>
		<pubDate>Thu, 21 Apr 2016 15:12:45 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Educação]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Shiny]]></category>

		<guid isPermaLink="false">./../../../index.html?p=946</guid>
		<description><![CDATA[Com essa emergência atual dos termos como Big Data, Data Science, Data Mining e afins, e também com a grande oferta de postos de trabalho que se abrem nesse &#8220;novo setor&#8221;, muitas pessoas se perguntam qual o melhor caminho para se formar e se preparar para esse mercado. Existem cursos a distância, oferecidos por plataformas como o Coursera e o Udacity, ou mesmo cursos em nível de pós-graduação, MAS surge a pergunta: existe alguma graduação...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">Com essa emergência atual dos termos como Big Data, Data Science, Data Mining e afins, e também com a grande oferta de postos de trabalho que se abrem nesse &#8220;novo setor&#8221;, muitas pessoas se perguntam qual o melhor caminho para se formar e se preparar para esse mercado. Existem cursos a distância, oferecidos por plataformas como o <a href="https://pt.coursera.org/specializations/jhu-data-science">Coursera</a> e o <a href="https://www.udacity.com/course/data-analyst-nanodegree--nd002">Udacity</a>, ou mesmo cursos em nível de pós-graduação, MAS surge a pergunta: existe alguma graduação que prepara o profissional para ser um cientista de dados?</p>
<p style="text-align: justify;">Na minha opinião, o curso de graduação que mais se aproxima daquilo que se deseja em um cientista de dados é o bacharelado em Estatística. Aqui eu estou usando o termo cientista de dados, não como de um profissional versado em tecnologias como <a href="http://hadoop.apache.org/">Hadoop</a>, <a href="http://spark.apache.org/">Spark</a>, <a href="https://www.mongodb.com/">MongoDB</a>, mas de um profissional que é preparado para trabalhar com análise de dados e planejamento de pesquisas.</p>
<p style="text-align: justify;">Claro, existe também uma gama de problemas relacionados a arquitetura e tecnologia, que eu acredito precisam do expertise de um profissional que talvez eu poderia chamar um Engenheiro de Dados. Mas em geral, um curso que prepara o profissional para trabalhar com o que hoje se apresenta como a Ciência de Dados, esse curso provavelmente é o curso de Estatística. Prova disso é que mesmo antes desse novo boom de Big Data, muitos profissionais de Estatística já vinham desempenhando o papel de um cientista de dados em empresas de diversos setores. Claro com dados em uma escala muito menor.</p>
<h2 style="text-align: justify;">Onde estudar Estatística?</h2>
<p style="text-align: justify;">Muitos estudantes do Ensino Médio não tem ideia de que existe o bacharelado em Estatística e muito menos onde cursa-lo no Brasil. Acredito que atualmente esse problema é menor que no passado, mas ainda assim a Estatística está longe de ser um curso tão popular quanto Ciência da Computação ou algumas Engenharias.</p>
<p style="text-align: justify;">Outro ponto fundamental é que não existem assim tantos locais para se estudar Estatística quanto se tem para estudar Engenharia tal que a escolha dos estudantes fica bastante limitada. De forma a agregar em um único local informações relacionados aos cursos de Estatística no Brasil, eu estou criando esse aplicativo em Shiny que vai condensar informações sobre todos os cursos de Estatística disponíveis no país. Inicialmente ele só conta com informações como a localização, a carga horária, se tem Mestrado ou Doutorado em Estatística, dentre outros, mas espero ir agregando mais e mais informações. No futuro planejo incluir também locais somente com pós-graduação.</p>
<h2 style="text-align: justify;">Aplicativo</h2>
<p>O aplicativo se encontra neste endereço:</p>
<p><a href="https://flaviobarros.shinyapps.io/onde-estudar-estatistica/">ONDE ESTUDAR ESTATÍSTICA?</a></p>
<p>Se você tem sugestões de como melhora-lo, informações extras que gostaria de ver, poste nos comentários, que na medida do possível vou ir tentando agregar.</p>
<p><a href="./../../../wp-content/uploads/2016/04/onde_estudar_estatistica.png"><img class="aligncenter size-full wp-image-947" src="./../../../wp-content/uploads/2016/04/onde_estudar_estatistica.png" alt="onde_estudar_estatistica" width="1244" height="394" srcset="./../../../wp-content/uploads/2016/04/onde_estudar_estatistica.png 1244w, ./../../../wp-content/uploads/2016/04/onde_estudar_estatistica-300x95.png 300w, ./../../../wp-content/uploads/2016/04/onde_estudar_estatistica-768x243.png 768w, ./../../../wp-content/uploads/2016/04/onde_estudar_estatistica-1024x324.png 1024w" sizes="(max-width: 1244px) 100vw, 1244px" /></a></p>
]]></content:encoded>
			<wfw:commentRss>./../../../2016/04/21/onde-estudar-estatistica/feed/index.html</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Impeachment &#8211; Análise das Intenções</title>
		<link>./../../../2016/04/04/impeachment-analise-das-intencoes/index.html</link>
		<comments>./../../../2016/04/04/impeachment-analise-das-intencoes/index.html#comments</comments>
		<pubDate>Mon, 04 Apr 2016 09:48:18 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Política]]></category>

		<guid isPermaLink="false">./../../../index.html?p=930</guid>
		<description><![CDATA[Recentemente eu li um artigo super interessante no blog do Regis A. Ely. Basicamente, ele utilizou os dados da pesquisa que o movimento &#8220;vem para a rua&#8221; está realizando sobre as intenções de votos no impeachment, para tentar criar um modelo de predição para a votação dos deputados indecisos. Eu achei super interessante, e pelo que eu vi, muita gente está compartilhando no Facebook. ENTRETANTO, eu fiquei curioso com relação a alguns pontos na análise...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">Recentemente eu li um artigo super interessante no blog do <a href="http://regisely.com/blog/impeachment/">Regis A. Ely</a>. Basicamente, ele utilizou os dados da pesquisa que o movimento &#8220;<a href="http://mapa.vemprarua.net/br/">vem para a rua</a>&#8221; está realizando sobre as intenções de votos no impeachment, para tentar criar um modelo de predição para a votação dos deputados indecisos. Eu achei super interessante, e pelo que eu vi, muita gente está compartilhando no Facebook.</p>
<p style="text-align: justify;">ENTRETANTO, eu fiquei curioso com relação a alguns pontos na análise e em relação às escolhas que ele fez. Minhas críticas em relação a análise são:</p>
<p style="text-align: justify;">1 &#8211; Será que somente o partido e o estado de origem são suficientes para fornecer uma boa previsão?</p>
<p style="text-align: justify;">2 &#8211; Qual o real desempenho do modelo?</p>
<p style="text-align: justify;">3 &#8211; Que outros insights os dados podem fornecer?</p>
<p style="text-align: justify;">4 &#8211; Os deputados já decididos representam bem os indecisos?</p>
<p style="text-align: justify;">A primeira questão é super importante, pois na verdade a ideia toda da análise é que um modelo onde eu sei somente o partido e o estado de origem do deputado (ou senador) é suficiente para fornecer uma boa previsão. O modelo também utiliza o pressuposto de que os deputados que já decidiram o voto representam bem todos os deputados, tal que uma vez que eu crie um modelo que aprenda a partir deles eu serei capaz de prever com segurança o voto dos outros, dos indecisos.</p>
<p style="text-align: justify;">A segunda questão me deixou bastante curioso, pois ele usou uma poda na árvore de decisão, mas pelo que eu vi ele não reportou o desempenho real em um pequeno conjunto de teste. Isso seria importante pois para confiar nas previsões de um modelo é bom saber aproximadamente qual será o desempenho do modelo.</p>
<p style="text-align: justify;">O terceiro ponto talvez seja o que eu acho que seria o mais importante de se fazer: uma pequena análise exploratória. Eu sei que as árvores de decisão são super interpretáveis, e que apesar de podermos pensar nelas como um modelo, &#8220;de uma certa forma&#8221; elas poderiam ser encaradas como um tipo de &#8220;análise exploratória&#8221;. ENTRETANTO, eu não gosto muito de criar modelos antes de &#8220;dar uma olhada&#8221; nos dados. E acredito que o principal resultado da análise, que foi mostrar como era a influência dos partidos, pode ser facilmente obtido por meio de algumas visualizações.</p>
<h2 style="text-align: justify;">Análise Exploratória</h2>
<p style="text-align: justify;">O que eu fiz foi basicamente pegar os dados no site do Regis e fazer algumas visualizações. Para não dizer que eu não adicionei nada, eu adicionei também a região e fiz um <em>scrape</em> no site do &#8220;vem pra rua&#8221; para incluir a variável sexo no conjunto de dados. Todos os resultados que eu vou apresentar se referem somente à câmara dos deputados.</p>
<p style="text-align: justify;">Minha primeira dúvida era com relação ao tamanho da bancada de cada partido na câmara, isto é, quais partidos tem mais deputados? <a href="./../../../wp-content/uploads/2016/04/tamanho_bancada.png" rel="attachment wp-att-938"><img class="aligncenter size-full wp-image-938" src="./../../../wp-content/uploads/2016/04/tamanho_bancada.png" alt="tamanho_bancada" width="480" height="480" srcset="./../../../wp-content/uploads/2016/04/tamanho_bancada.png 480w, ./../../../wp-content/uploads/2016/04/tamanho_bancada-150x150.png 150w, ./../../../wp-content/uploads/2016/04/tamanho_bancada-300x300.png 300w, ./../../../wp-content/uploads/2016/04/tamanho_bancada-65x65.png 65w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<p style="text-align: justify;">Muita gente que não acompanha a política as vezes fica surpresa com o tamanho da bancada do PMDB, mas o FATO é que o PMDB é o maior partido da câmara e é justamente esse partido que deve decidir o impeachment. Veja que o PT tem a segunda maior bancada e pasmem para o tamanho do PP e do PR&#8230;(sim o PP é do Paulo Maluf!)</p>
<p style="text-align: justify;">Depois que eu vi esse gráfico logo pensei: ok, o PT deve votar em massa contra, o PSDB em massa a favor, mas e o resto? Como está até agora a distribuição? Vamos ver!</p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/04/bancada_voto.png" rel="attachment wp-att-937"><img class="aligncenter size-full wp-image-937" src="./../../../wp-content/uploads/2016/04/bancada_voto.png" alt="bancada_voto" width="480" height="480" srcset="./../../../wp-content/uploads/2016/04/bancada_voto.png 480w, ./../../../wp-content/uploads/2016/04/bancada_voto-150x150.png 150w, ./../../../wp-content/uploads/2016/04/bancada_voto-300x300.png 300w, ./../../../wp-content/uploads/2016/04/bancada_voto-65x65.png 65w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<p style="text-align: justify;">Esse é praticamente o mesmo gráfico anterior, mas com a informação adicional de como está divido o partido. Veja que o PMDB está rachado: além de ter uma parcela que é contra, o partido ainda tem uma parte significativa de indecisos. A forma como o PMDB vai votar vai selar o destino da Presidente Dilma. O PT naturalmente vota contra em massa, e sendo a segunda bancada isso pesa fortemente a favor da presidente, mas o PP, o PR, o PSD, PRB e PDT ainda tem um contingente enorme de indecisos. É por isso que a presidente está distribuindo ministérios a rodo na tentativa de angariar esse votos. Um desembarque do PP do governo também pode ser um golpe de misericórdia.</p>
<p style="text-align: justify;">Além do partido do deputado, o estado de origem também é uma variável que vai entrar no modelo. Vamos ver como anda a distribuição em relação ao estado e a região.</p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/04/bancada_estado.png" rel="attachment wp-att-934"><img class="aligncenter size-full wp-image-934" src="./../../../wp-content/uploads/2016/04/bancada_estado.png" alt="bancada_estado" width="480" height="480" srcset="./../../../wp-content/uploads/2016/04/bancada_estado.png 480w, ./../../../wp-content/uploads/2016/04/bancada_estado-150x150.png 150w, ./../../../wp-content/uploads/2016/04/bancada_estado-300x300.png 300w, ./../../../wp-content/uploads/2016/04/bancada_estado-65x65.png 65w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/04/bancada_regiao.png" rel="attachment wp-att-935"><img class="aligncenter size-full wp-image-935" src="./../../../wp-content/uploads/2016/04/bancada_regiao.png" alt="bancada_regiao" width="480" height="480" srcset="./../../../wp-content/uploads/2016/04/bancada_regiao.png 480w, ./../../../wp-content/uploads/2016/04/bancada_regiao-150x150.png 150w, ./../../../wp-content/uploads/2016/04/bancada_regiao-300x300.png 300w, ./../../../wp-content/uploads/2016/04/bancada_regiao-65x65.png 65w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<p style="text-align: justify;">Vejam que o estado de São Paulo tem uma quantidade muito grande votos a favor. Isso se deve principalmente ao PSDB que é forte no estado e tem uma bancada que vota em massa no impeachment. NO ENTANTO, na Bahia, no Rio, em Minhas e em São Paulo mesmo, ainda há muitos indecisos. Esses estados vão ter um papel fundamental.  Vejam que o Sul e o Centro-Oeste votam em peso a favor, e tanto o nordeste quanto o norte ainda estão bastante divididos.</p>
<p style="text-align: justify;">Uma questão que eu fiquei intrigado, e para ser honesto eu não sabia de antemão, eram quantas mulheres são deputadas na câmara. Mais que isso, eu fiquei me perguntando se talvez as deputadas teriam  uma distribuição diferente em relação aos homens. Primeiramente gostaria de destacar que existem muito poucas mulheres na câmara (54 sendo que 2 não estão em exercício) e ainda que elas votassem diferente, ainda assim, provavelmente não teriam um impacto tão grande. Nos modelos que eu testei o sexo de fato não teve um peso grande. MAS vamos ver a distribuição:</p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/04/bancada_sexo.png" rel="attachment wp-att-936"><img class="aligncenter size-full wp-image-936" src="./../../../wp-content/uploads/2016/04/bancada_sexo.png" alt="bancada_sexo" width="480" height="480" srcset="./../../../wp-content/uploads/2016/04/bancada_sexo.png 480w, ./../../../wp-content/uploads/2016/04/bancada_sexo-150x150.png 150w, ./../../../wp-content/uploads/2016/04/bancada_sexo-300x300.png 300w, ./../../../wp-content/uploads/2016/04/bancada_sexo-65x65.png 65w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<p style="text-align: justify;">Bem divido, como no caso dos homens.</p>
<h2 style="text-align: justify;">Modelagem</h2>
<p style="text-align: justify;">Para a etapa de modelagem eu fiz uma amostra aleatória de 95 observações (3/4) que eu deixei para teste. Daí eu criei 4 modelos, com as 289 observações restantes, utilizando árvores de decisão, regressão logística, randomForest e Gradiente Boosting. A especificação foi: Estado + Partido + Estado:Partido. ISTO É, as duas variáveis mais o efeito de interação. A ideia da interação é porque, de repente, um deputado, mesmo sendo do mesmo partido, vota diferente dependendo da região. Essa hipótese é plausível e por isso eu coloquei.</p>
<p style="text-align: justify;">A seguir o resultado do ajuste do RandomForest (só um exemplo) e  a seguir eu apresento as acurácias globais e o intervalo de confiança 95% para a acurácia de predição no conjunto de teste. Vejam que eu deixei de fora todos os deputados indecisos. Portanto o ajuste foi com uma variável resposta com duas classes, contra e a favor.</p>
<pre class="lang:r decode:true ">Random Forest 

289 samples
  8 predictors
  2 classes: 'A favor', 'Contra' 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 260, 260, 260, 260, 261, 260, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD 
    2   0.6885468  0.0000000  0.003504988  0.0000000
   35   0.9066502  0.7614794  0.039824877  0.1142898
  647   0.9135468  0.7786241  0.046595216  0.1308177

Accuracy was used to select the optimal model using  the
 largest value.
The final value used for the model was mtry = 647.</pre>
<p style="text-align: justify;">Eu estou utilizando o pacote caret, tal que o modelo é ajustado com os melhores hipeparâmetros obtidos por validação cruzada. Segundo o resultado obtido por validação cruzada, o desempenho do modelo deve ficar na casa dos 90%. Vamos verificar isso no conjunto de teste.</p>
<pre class="lang:r decode:true ">Confusion Matrix and Statistics

          Reference
Prediction A favor Contra
   A favor      63      8
   Contra        3     21
                                          
               Accuracy : 0.8842          
                 95% CI : (0.8023, 0.9408)
    No Information Rate : 0.6947          
    P-Value [Acc &gt; NIR] : 1.207e-05       
                                          
                  Kappa : 0.7131          
 Mcnemar's Test P-Value : 0.2278          
                                          
            Sensitivity : 0.9545          
            Specificity : 0.7241          
         Pos Pred Value : 0.8873          
         Neg Pred Value : 0.8750          
             Prevalence : 0.6947          
         Detection Rate : 0.6632          
   Detection Prevalence : 0.7474          
      Balanced Accuracy : 0.8393          
                                          
       'Positive' Class : A favor</pre>
<p style="text-align: justify;">Vejam que a sensibilidade é alta, mas a especificidade não (a classe de referência é o voto a favor). Isso indica que o modelo é bom em detectar o voto a favor, mas não tão bom para detectar o voto contra. Isso deve estar acontecendo, dentro outras razões, poque o conjunto é desbalanceado em relação aos votos a favor. O CERTO seria utilizar alguma medida para corrigir isso, como: alterar o valor da probabilidade de corte a partir da curva ROC, oversampling e undersampling, ou mesmo aprendizado sensível ao erro na classe minoritária. Aí você pergunta:  Por que você não fez isso?!! EU RESPONDO: provavelmente o partido e o estado sozinhos não devem fornecer informação suficiente para saber o voto do deputado e também provavelmente os decididos também não devem representar tão bem os indecisos&#8230;LOGO não valia tanto a pena perder tempo com isso. Eu só queria mesmo dar uma olhada e tentar obter alguns valores que eu não vi no post do Regis.</p>
<p><a href="./../../../wp-content/uploads/2016/04/ICs.png" rel="attachment wp-att-939"><img class="aligncenter size-full wp-image-939" src="./../../../wp-content/uploads/2016/04/ICs.png" alt="ICs" width="480" height="480" srcset="./../../../wp-content/uploads/2016/04/ICs.png 480w, ./../../../wp-content/uploads/2016/04/ICs-150x150.png 150w, ./../../../wp-content/uploads/2016/04/ICs-300x300.png 300w, ./../../../wp-content/uploads/2016/04/ICs-65x65.png 65w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<p>Outra dúvida que eu tinha ficado é se utilizar outras técnicas traria algum ganho, e pelo intervalos de confiança acima, considerando os quatro métodos, parece que não. Veja que existem vários valores plausíveis em comum para a acurácia global tal que eu não alegaria que nenhum método é melhor que o outro. Vejam que de fato o ranfomForest teve um desempenho no teste em torno de 90%.</p>
<p style="text-align: justify;">Por fim, um modelo como o randomForest, apesar de ser um ensemble, provê uma medida, em termos de ganho de informação, com a qual é possível ter uma ideia das importâncias das variáveis:</p>
<p><a href="./../../../wp-content/uploads/2016/04/importance.png" rel="attachment wp-att-940"><img class="aligncenter size-full wp-image-940" src="./../../../wp-content/uploads/2016/04/importance.png" alt="importance" width="480" height="480" srcset="./../../../wp-content/uploads/2016/04/importance.png 480w, ./../../../wp-content/uploads/2016/04/importance-150x150.png 150w, ./../../../wp-content/uploads/2016/04/importance-300x300.png 300w, ./../../../wp-content/uploads/2016/04/importance-65x65.png 65w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<p style="text-align: justify;">Selecionei somente as 10 mais importantes e naturalmente os partidos PT, PCdoB e etc, tem uma influência muito grande. Você pode pensar da seguinte maneira: se eu sei que um deputado é deste partido com certeza ele vota de um dos lados. O RF infelizmente não indica de que forma é essa influência, mas como fizemos uma análise exploratória, é fácil saber que quando um deputado é do PT ele vota contra o impeachment. Vejam que interessante as interações entre PMDB e o estado do PA. Isso deve ser sinal que muitos deputados daquele estado, que são do PMDB, provavelmente votam mais de um lado do que do outro.</p>
<h2>Previsões</h2>
<p style="text-align: justify;">Agora que temos alguns modelos, vamos utilizar o randomForest e fazer algumas previsões para os indecisos. De acordo com esse modelo, teremos 66 deputados a favor e 30 contra, tal que somando o total de 265 a favor que já existem, temos um total de 331 dos 513 que devem votar a favor do impeachment. Isso dá aproximadamente 64% da câmara, tal que nesse caso a presidente sofreria o impeachment.</p>
<h2 style="text-align: justify;">Conclusão</h2>
<p style="text-align: justify;">Dá para acreditar nessa previsão? EU DIRIA QUE NÃO. Isso ocorre por várias razões, mas dentre elas eu destacaria o fato de que não é só o partido e o estado que orientam a decisão de um deputado. Existem deputados na câmara com muito mais influência, tal que o voto de muitos pode estar condicionado a decisão do líder de um grupo. ASSIM, talvez seja necessário incluir alguma medida de influência no modelo. Talvez uma abordagem mais confiável seria verificar os grupos de deputados que tem votado junto recentemente, isto é, tentar prever o voto dos indecisos não com variáveis como o partido ou o estado, mas utilizando os &#8220;vizinhos mais próximos&#8221; em termos de perfil de votação. Mas de qualquer forma os gráficos revelam mais ou menos como está a câmara e meu palpite é que a presidente não sobrevive ao impeachment, PELO MENOS NA CÂMARA.</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2016/04/04/impeachment-analise-das-intencoes/feed/index.html</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
		<item>
		<title>RECONHECIMENTO DE DÍGITOS ESCRITOS A MÃO – PARTE 3</title>
		<link>./../../../2016/03/14/reconhecimento-de-digitos-escritos-mao-parte-3/index.html</link>
		<comments>./../../../2016/03/14/reconhecimento-de-digitos-escritos-mao-parte-3/index.html#comments</comments>
		<pubDate>Tue, 15 Mar 2016 02:54:32 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Aprendizado de Máquina]]></category>
		<category><![CDATA[Data Mining]]></category>
		<category><![CDATA[Educação]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[Preparação de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">./../../../index.html?p=851</guid>
		<description><![CDATA[Na Parte 1 desse post (que já publiquei faz um tempão!) eu fiz uma classificação de imagens de dígitos escritos a mão usando o k-nn (algoritmo dos vizinhos mais próximos) usando as informações das imagens sem nenhum tipo de tratamento, isto é, sem nenhum método de preparação. Como foi mostrado, o k-nn só foi capaz de classificar razoavelmente bem com com k = 1 e conseguiu uma acurácia de apenas 78%, algo muito distante do que...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">Na <a href="./../../../2014/12/22/reconhecimento-de-digitos-escritos-mao-parte-1/index.html">Parte 1</a> desse post (que já publiquei faz um tempão!) eu fiz uma classificação de imagens de dígitos escritos a mão usando o <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nn</a> (algoritmo dos vizinhos mais próximos) usando as informações das imagens sem nenhum tipo de tratamento, isto é, sem nenhum método de preparação. Como foi mostrado, o <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nn</a> só foi capaz de classificar razoavelmente bem com com k = 1 e conseguiu uma acurácia de apenas 78%, algo muito distante do que ainda pode ser conseguido.</p>
<p style="text-align: justify;">Na <a href="./../../../2015/09/14/reconhecimento-de-digitos-escritos-a-mao-parte-2/index.html">Parte 2</a> eu trabalhei com um método de redução de dimensionalidade, o PCA, e também foram explorados diversos outros classificadores, como o Random Forest, o SVM e etc. O resumo dos resultados foi o seguinte:</p>
<ul style="text-align: justify;">
<li style="text-align: justify;">k-nn com k = 1: 84%</li>
<li style="text-align: justify;">Regressão linear: 14%</li>
<li style="text-align: justify;">Regressão Logística Multinomial: 64%</li>
<li style="text-align: justify;">Árvores de decisão: 24%</li>
<li style="text-align: justify;">RandomForest: 72%</li>
</ul>
<p style="text-align: justify;">E a conclusão geral foi que não foi possível bater o k-NN ou ainda mais chegar aos resultados reportados na literatura, superiores a 95% de acurácia na tarefa. Como foi mencionado anteriormente, os possíveis problemas foram:</p>
<ol style="text-align: justify;">
<li>Conjunto pequeno de imagens.</li>
<li>Modelos com parâmetros default.</li>
</ol>
<p style="text-align: justify;">Assim, nessa última parte vou mostrar como é possível treinar melhores modelos com muito mais dados e como é possível melhorar a performance dos algoritmos com melhores hiperparâmetros. Outro ponto importante que eu queria mostrar também é o uso do pacote <a href="http://caret.r-forge.r-project.org/">caret</a> para automatizar diversas tarefas desse processo.</p>
<h2 style="text-align: justify;">1.Dados e visualização</h2>
<p style="text-align: justify;">Como eu comentei antes, vamos nessa parte tentar utilizar um conjunto maior de imagens. Para tanto, ao invés desse pequeno conjunto de teste e treino que foi fornecido na <a href="./../../../2014/12/22/reconhecimento-de-digitos-escritos-mao-parte-1/index.html">Parte 1</a>, vamos utilizar um conjunto muito maior de images de dígitos escritos a mão, o famoso data set <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>. Se você verificar nesse link, você vai encontrar diversas informações com respeito a esse conjunto de dados e também você pode baixa-lo e utilizar nas análises que se seguem, caso queira reproduzir o que você está vendo aqui. ENTRETANTO, para poupar o seu trabalho e o meu, ao invés de pegar os dados diretamente deste site, vamos utilizar esse mesmo conjunto de dados já tratado e preparado pela equipe do <a href="https://www.kaggle.com/c/digit-recognizer">Kaggle</a>. A vantagem é que o arquivos já estão no formato csv e não será mais necessária a etapa de preparação realizada na <a href="./../../../2014/12/22/reconhecimento-de-digitos-escritos-mao-parte-1/index.html">Parte 1</a> e na <a href="./../../../2015/09/14/reconhecimento-de-digitos-escritos-a-mao-parte-2/index.html">Parte 2</a> desta série. Outra vantagem é que após você rodar estes modelos, se você quiser, você pode submeter seus resultados no Leaderboard para experimentar como funciona esse site de competição.</p>
<p style="text-align: justify;">Na <a href="./../../../2015/09/14/reconhecimento-de-digitos-escritos-a-mao-parte-2/index.html">Parte 2</a>, como eu peguei diretamente as imagens e converti em uma matriz, agora você poderia ficar confuso e se perguntar: mas e aí, como são estas imagens? como eu vou ver se já está em csv? Para você não ficar com dúvida, vamos &#8220;imprimir&#8221; as imagens.</p>
<pre class="lang:r decode:true ">############ Explorando as imagens ###############################
## Contando o número de imagens por dígito
barplot(table(treino$label), ylim = c(0,5000))

## Transformando em uma matriz
treino &lt;- as.matrix(treino)

## Imprimindo uma imagem
matriz_imagem &lt;- matrix(treino[1000,-1], ncol = 28)
matriz_imagem &lt;- matriz_imagem[,28:1] ## invertendo a imagem
image(1:28, 1:28, matriz_imagem, col = c('white', 'black'))</pre>
<p><a href="./../../../wp-content/uploads/2016/03/digito4.png" rel="attachment wp-att-867"><img class="aligncenter size-full wp-image-867" src="./../../../wp-content/uploads/2016/03/digito4.png" alt="digito4" width="480" height="480" srcset="./../../../wp-content/uploads/2016/03/digito4.png 480w, ./../../../wp-content/uploads/2016/03/digito4-150x150.png 150w, ./../../../wp-content/uploads/2016/03/digito4-300x300.png 300w, ./../../../wp-content/uploads/2016/03/digito4-65x65.png 65w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<p>Veja que eu peguei a primeira linha do conjunto de treino, transformei em uma matriz e imprimi a matriz como uma imagem. Nesse caso é o dígito 4. Assim, apesar de agora você estar usando um arquivo em csv preparado eles fizeram a mesma coisa que eu fiz anteriormente. Se você quiser entender melhor como cada imagem virou uma linha dessa tabela dá uma olhada na Parte 1 dessa série. Só para mostrar que os dígitos estão ok, eu vou imprimir uma &#8220;imagem média&#8221;, onde em cada imagem eu tenho um valor médio em cada píxel considerando todas as imagens do conjunto de dados.</p>
<pre class="lang:r decode:true ">## Plotando uma imagem média para cada dígito
## Definindo uma escala de cor, indo do branco ao preto
colors &lt;- c('white','black')
cus_col &lt;- colorRampPalette(colors=colors)

## Plot de cada imagem média
## Divindo a tela
png('todos_digitos.png')
par(mfrow=c(4,3),pty='s',mar=c(1,1,1,1),xaxt='n',yaxt='n')

## Criando um array para armazenar as matrizes de cada imagem média
all_img &lt;- array(dim=c(10,28*28))

## Recuperando todas as imagens por dígito e calculando a média
for(di in 0:9) {
  print(di)
  all_img[di+1,] &lt;- apply(treino[treino[,1]==di,-1],2,sum)
  all_img[di+1,] &lt;- all_img[di+1,]/max(all_img[di+1,])*255
  
  z&lt;-array(all_img[di+1,],dim=c(28,28))
  z&lt;-z[,28:1] ##right side up
  image(1:28,1:28,z,main=di,col=cus_col(256))
}</pre>
<p><a href="./../../../wp-content/uploads/2016/03/todos_digitos-1.png" rel="attachment wp-att-869"><img class="aligncenter size-full wp-image-869" src="./../../../wp-content/uploads/2016/03/todos_digitos-1.png" alt="todos_digitos" width="480" height="480" srcset="./../../../wp-content/uploads/2016/03/todos_digitos-1.png 480w, ./../../../wp-content/uploads/2016/03/todos_digitos-1-150x150.png 150w, ./../../../wp-content/uploads/2016/03/todos_digitos-1-300x300.png 300w, ./../../../wp-content/uploads/2016/03/todos_digitos-1-65x65.png 65w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<p style="text-align: justify;">Até há uma certa variação (por isso que há uma sombra) mas no geral os mesmos píxels tem uma intensidade maior considerando cada dígito diferente que foi escrito na imagem. Isso nos leva a crer que os modelos devem conseguir distinguir um dígito do outro.</p>
<h2 style="text-align: justify;">2. Preparação com PCA</h2>
<p style="text-align: justify;">Depois que você baixar os arquivos train.csv e test.csv do Kaggle já podemos efetuar a leitura dos arquivos e a preparação por meio do PCA. O que vamos fazer é aplicar o PCA e retendo somente o número de componentes necessário para alcançar 95% da variância total. Os detalhes sobre isso eu discuti na Parte 2.</p>
<pre class="lang:r decode:true ">## Leitura dos conjuntos de dados de treino e de teste
treino = read.csv('train.csv', header = T)
teste = read.csv('test.csv', header = T)

############ APlicação do PCA ####################################
## Obtendo componentes principais
pc &lt;- prcomp(treino[,-1])
treino_pc &lt;- pc$x

## Obtendo as variâncias acumuladas
vars = pc$sdev^2
props = vars/sum(vars)
varAcum = cumsum(props)
which.min(varAcum &lt; 0.90)

## Aplicando a rotação nos dados de teste
teste_pc &lt;- predict(pc, newdata = teste)

## Salvando treino e teste com PCA
save(treino_pc, file = 'treino_pc.rda')
save(teste_pc, file = 'teste_pc.rda')</pre>
<p style="text-align: justify;">eu costumo salvar os arquivos após cada etapa de preparação de forma a não precisar realizar o processo posteriormente. Outro ponto importante é que salvando os objetos no formato nativo do R, caso você precise recarregar os dados, o processo é muito mais rápido que a leitura em csv.</p>
<h2 style="text-align: justify;">3. Árvore de Decisão</h2>
<p style="text-align: justify;">Como uma primeira tentativa, vamos utilizar o algoritmo para árvores de decisão do pacote <a href="https://cran.r-project.org/web/packages/rpart/index.html">rpart</a>. Vamos utilizar todas as PC&#8217;s e treinar a árvore em 3/4 dos dados. O teste será realizado no 1/4 que foi separado.</p>
<pre class="lang:r decode:true ">#################################################################
## Teste com árvore de decisão
library(rpart)

## Separando o conjunto treino em dois para avaliação
set.seed(1)
inTrain &lt;- createDataPartition(treino$label, p = 3/4, list = F)
train &lt;- treino_pc[inTrain,]
evaluation &lt;- treino_pc[-inTrain,]

## Data frame de treino e teste
treino_arvore = as.data.frame(cbind(train[,1:784]))
treino_arvore$classes = as.factor(treino$label[inTrain])

## Conjunto de teste para avaliação
teste_arvore = as.data.frame(evaluation[,1:784])

## Criando uma árvore
arvore &lt;- rpart(classes ~ ., data = treino_arvore)

## Calculando a matriz de confusão
confusionMatrix(predict(arvore, teste_arvore, type = 'class'), treino$label[-inTrain])
</pre>
<p>e o resultado da matriz de confusão:</p>
<pre class="lang:r decode:true ">Confusion Matrix and Statistics

          Reference
Prediction   0   1   2   3   4   5   6   7   8   9
         0 637   2  13  20   3  66  17  12   0   2
         1   0 995   9   7  28   7   3  75   6  53
         2  60  54 741  61  30  97 130  16  93  10
         3 185  45 111 803  13 270 106  29  89  30
         4   3   0  16  10 796  73  16  82  16 471
         5  86  13  46  88  36 341  31  61 185  49
         6  24  16  47  45  21  40 725   4  13  43
         7  17   0   4   2  20  16   2 652  10  85
         8  18  46  40  27  11  60  12  18 532  13
         9  14   0   6  10  74   6  13 103  57 305

Overall Statistics
                                         
               Accuracy : 0.6217         
                 95% CI : (0.6124, 0.631)
    No Information Rate : 0.1115         
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
                                         
                  Kappa : 0.5796         
 Mcnemar's Test P-Value : &lt; 2.2e-16</pre>
<p style="text-align: justify;">mostra que o desempenho ainda está longe do satisfatório. Com uma acurácia global de apenas 62% estamos ainda muito longe da meta de 95%. Veja que utilizamos a estratégia <a href="http://stats.stackexchange.com/questions/104713/hold-out-validation-vs-k-fold-validation">holdout</a>, e com relação a <a href="./../../../2015/09/14/reconhecimento-de-digitos-escritos-a-mao-parte-2/index.html">Parte 2</a> desta série a única mudança é o fato de estarmos trabalhando com imagens com mais resolução e um conjunto maior. Parece que isso ainda não é o suficiente, assim vamos explorar outros algoritmos e vamos utilizar métodos de validação cruzada para encontrar os melhores hiperparâmetros.</p>
<h2>4. RandomForest</h2>
<p style="text-align: justify;">O <a href="https://en.wikipedia.org/wiki/Random_forest">randomforest</a> é um dos algoritmos de machine learning <a href="https://www.quora.com/What-are-the-top-10-data-mining-or-machine-learning-algorithms">mais utilizados na indústria</a>. Seu sucesso advém do fato de ser robusto, facilmente paralelizável e apresentar um desempenho muito bom em uma grande quantidade de problemas diferentes. Assim, vamos experimentar esse classificador procurando ajustar os melhores hiperperâmetros por validação cruzada. No caso do RF temos que definir qual o melhor m, um parâmetro que determina quantas variáveis são sorteadas na escolha do split em cada nó, de cada árvore de decisão do comitê. Se não ficou claro para você o que significa este hiperparâmetro não tem problema, não é difícil encontrar material onde você pode entender os detalhes do RF. O importante aqui é você entender que o valor do hiperparâmetro será escolhido com base no próprio conjunto de dados, utilizando validação cruzada.</p>
<pre class="lang:r decode:true">#################################################################
## Teste com RandomForest
library(randomForest)

## Separando o conjunto treino em dois para avaliação
set.seed(1)
inTrain &lt;- createDataPartition(treino$label, p = 3/4, list = F)
train &lt;- treino_pc[inTrain,]
evaluation &lt;- treino_pc[-inTrain,]

## Data frame de treino e teste, aqui retendo somente 160 PC's, equivalente a 95% de ## variância.
treino_arvore = as.data.frame(cbind(train[,1:160]))
treino_arvore$classes = as.factor(treino$label[inTrain])

## Conjunto de teste para avaliação
teste_arvore = as.data.frame(evaluation[,1:160])

## Modelagem
fitControl &lt;- trainControl(method = "oob", verboseIter = T,
                           
                           ## Estimate class probabilities
                           classProbs = F)

set.seed(825)
rfFit &lt;- train(classes ~ ., data = treino_arvore, verbose = T,
                method = "rf",
                trControl = fitControl,
                tuneLength = 8,
                metric = "Accuracy")
save(rfFit, file = 'rfFit.rda')
                
## Calculando a matriz de confusão
confusionMatrix(predict(rfFit, teste_arvore, type = 'raw'), treino$label[-inTrain])
</pre>
<p>e a matriz de confusão:</p>
<pre class="lang:r decode:true ">Confusion Matrix and Statistics

          Reference
Prediction    0    1    2    3    4    5    6    7    8    9
         0 1048    0    3    0    1    1    1    0    0    1
         1    0 1130    2    0    1    1    0    2    3    0
         2    0    3 1021   10    4    2    1    6    2    0
         3    0    1    5 1073    3   12    0    1    8   11
         4    1    0    6    0  970    4    1    1    4   18
         5    0    0    0    8    2  932    6    0    4    6
         6    6    1    3    2    5    3 1023    1    2    0
         7    1    1    4    4    0    0    0 1077    3    7
         8    0    0    5    4    5    2    1    2  967    4
         9    2    2    3    2   11    2    0    1    2 1020

Overall Statistics
                                          
               Accuracy : 0.9774          
                 95% CI : (0.9744, 0.9802)
    No Information Rate : 0.1084          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
                                          
                  Kappa : 0.9749</pre>
<p>e enfim chegamos a 97%!</p>
<h2 style="text-align: justify;">5. SVM (Máquina de vertores de suporte)</h2>
<p style="text-align: justify;">O SVM é um algoritmo do tipo &#8220;caixa preta&#8221;. O princípio por detrás do algoritmo é criar hiperplanos separadores em dimensões maiores do que as presentes no conjunto de dados. A ideia é que se os pontos são linearmente separáveis, isto é, se um hiperplano como fronteira de decisão conseguiria separar completamente as classes, então o SVM é um método que pode ser utilizado para encontrar esse hiperplano. ENTRETANTO, ocorre que muitos problemas não são linearmente separáveis, e ainda que fossem não valeria a pena usar o SVM. Quando o problema não é linearmente separável o SVM, de uma certa forma, projeta os dados em um espaço onde é possível criar um hiperplano separador. Também, ele aceita um certo grau de &#8220;impurezas&#8221; dentro das fronteiras de decisão. Enfim, é um algoritmo do tipo &#8220;caixa preta&#8221;, que não tem origem na estatística já que é basicamente um algoritmo de otimização. Mas o fato é que o SVM apresenta resultados muito bons e uma grande quantidade de problemas e vamos ver isso aqui nesse teste.</p>
<pre class="lang:r decode:true">#################################################################
## Teste com SVM RBF
## Carregando os pacotes
library(caret)

## Modelagem
fitControl &lt;- trainControl(method = "cv", verboseIter = T,
                           
                           ## Estimate class probabilities
                           classProbs = F)

set.seed(825)
svmFit &lt;- train(classes ~ ., data = treino_arvore,
                method = "svmRadial",
                trControl = fitControl,
                preProc = c("center", "scale"),
                tuneLength = 8,
                metric = "Accuracy")
save(svmFit, file = 'svmFit2.rda')

## Calculando a matriz de confusão
confusionMatrix(predict(svmFit, teste_arvore, type = 'raw'), treino$label[-inTrain])
</pre>
<p>e avaliando no conjunto de avaliação:</p>
<pre class="lang:r decode:true ">Confusion Matrix and Statistics

          Reference
Prediction    0    1    2    3    4    5    6    7    8    9
         0 1046    0    3    0    1    3    5    1    3    6
         1    1 1128    1    2    2    0    1    3    2    1
         2    0    3 1015   13    4    3    1   15    4    1
         3    1    1    4 1058    0   13    0    1    7    9
         4    2    2   10    1  972    1    4   10    1   21
         5    0    1    2   12    0  924    7    0    3    5
         6    5    0    2    1    5   10 1010    0    4    0
         7    0    0    5    7    0    0    0 1051    0   13
         8    3    2    9    4    0    2    5    1  967    3
         9    0    1    1    5   18    3    0    9    4 1008

Overall Statistics
                                          
               Accuracy : 0.9696          
                 95% CI : (0.9661, 0.9728)
    No Information Rate : 0.1084          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
                                          
                  Kappa : 0.9662</pre>
<p>e novamente conseguimos algo em torno de 97%. Para ser honesto, depois que eu criei os modelos com os melhores hiperparâmetros, submetendo no Kaggle o SVM supera os 98%. Foi o modelo com melhor desempenho nessa tarefa.</p>
<h2>Conclusão</h2>
<p style="text-align: justify;">Acho que depois da Parte 1, Parte 2 e Parte 3 (esta aqui!) você viu como se trabalha com classificação de imagens, como se prepara esse tipo de dado e como é possível alcançar altíssima acurácia utilizando os modelos de machine learning que você encontra por aí. Se você for reproduzir os exemplos, fique ciente que a etapa de modelagem pode demorar muito. No meu caso alguns destes modelos demoraram mais de 8 horas para o ajuste com os melhore hiperparâmetros! Outro ponto que eu não abordei é como o caret seleciona os melhores hiperparâmetros por validação cruzada. Isso vou deixar para falar com maior detalhe em outra oportunidade. Também gostaria de salientar que uma modelagem como essa é algo tipicamente diferente do que se espera em uma análise estatística tradicional. Aqui não estávamos interessados em inferência, mas sim em produzir modelos com o maior poder preditivo possível. Em casos como esse, trabalhar com métodos &#8220;caixa preta&#8221; não é em si um problema. O ponto principal é ter certeza que seu modelo apresentará um bom desempenho no futuro, com novos dados. POR FIM, esses modelos não são o estado da arte nesta tarefa, já que com <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> é possível passar dos 99% de acurácia.</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2016/03/14/reconhecimento-de-digitos-escritos-mao-parte-3/feed/index.html</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Livros recomendados &#8211; Data Science</title>
		<link>./../../../2016/03/12/livros-recomendados-data-science/index.html</link>
		<comments>./../../../2016/03/12/livros-recomendados-data-science/index.html#comments</comments>
		<pubDate>Sat, 12 Mar 2016 17:46:52 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Aprendizado de Máquina]]></category>
		<category><![CDATA[Educação]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">./../../../index.html?p=858</guid>
		<description><![CDATA[Ao longo dos últimos anos, trabalhando com pesquisa na pós-graduação, como estudante de Estatística e como um analista, eu venho consultando e estudando diversos materiais, de artigos em papers até livros sobre Data Mining, Data Science, Estatística, Big Data e etc. Eu tive oportunidade de consultar muitos bons livros, alguns menos e muitos que eram realmente ruins. ASSIM, nesse post eu gostaria de apresentar a minha seleção de livros e uma breve explicação de porque...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">Ao longo dos últimos anos, trabalhando com pesquisa na pós-graduação, como estudante de Estatística e como um analista, eu venho consultando e estudando diversos materiais, de artigos em <em>papers</em> até livros sobre Data Mining, Data Science, Estatística, Big Data e etc. Eu tive oportunidade de consultar muitos bons livros, alguns menos e muitos que eram realmente ruins. ASSIM, nesse post eu gostaria de apresentar a minha seleção de livros e uma breve explicação de porque eu gosto deles e o que você pode encontrar nesses materiais. Estou falando de livros na perspectiva de alguém que trabalha com aplicações, mas não é uma revisão extensiva da literatura da área. Quem visita esse blog já deve ter percebido que eu uso bastante o R e de fato minha lista tem um certo viés indicando livros que usam essa ferramenta. Vamos aos livros!</p>
<h2 style="text-align: justify;">1. Probabilidade e Estatística</h2>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/03/devore.jpg" rel="attachment wp-att-859"><img class="wp-image-859 alignleft" src="./../../../wp-content/uploads/2016/03/devore.jpg" alt="capa Estatística_cengage.cdr" width="137" height="185" srcset="./../../../wp-content/uploads/2016/03/devore.jpg 600w, ./../../../wp-content/uploads/2016/03/devore-222x300.jpg 222w" sizes="(max-width: 137px) 100vw, 137px" /></a> Eu consultava esse livro quando estava estudando Inferência I no bacharelado em Estatística. Não é um livro que geralmente é utilizado em cursos de Estatística (esse não estava na bibliografia) pois apresenta a estatística básica em um nível mais conceitual, praticamente sem demonstrações. JUSTAMENTE POR ISSO eu acho uma excelente indicação para entender estatística básica. TODOS os exemplos e exercícios são com dados reais, de pesquisas reais, nas áreas de engenharia e ciências, o que dá um gostinho a mais já que o leitor consegue ver exatamente como a estatística é aplicada na vida real. O livro cobre probabilidade, testes de hipótese, IC&#8217;s, teste de independência, ANOVA e etc. Está tudo aí, apresentado de forma muito intuitiva sempre com vistas nas aplicações. Um livro realmente muito bom, que para mim na época, fazendo um curso teórico de inferência, trouxe bastante da intuição sobre os métodos que eu estava estudando.</p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/03/uma_senhora_toma_cha.jpg" rel="attachment wp-att-862"><img class=" wp-image-862 alignleft" src="./../../../wp-content/uploads/2016/03/uma_senhora_toma_cha.jpg" alt="capa - uma senhora toma cha_18-02-10.indd" width="140" height="201" srcset="./../../../wp-content/uploads/2016/03/uma_senhora_toma_cha.jpg 1315w, ./../../../wp-content/uploads/2016/03/uma_senhora_toma_cha-209x300.jpg 209w, ./../../../wp-content/uploads/2016/03/uma_senhora_toma_cha-768x1104.jpg 768w, ./../../../wp-content/uploads/2016/03/uma_senhora_toma_cha-712x1024.jpg 712w" sizes="(max-width: 140px) 100vw, 140px" /></a></p>
<p style="text-align: justify;">Quando eu comecei a ler este livro eu mal conseguia parar! É um livro que começa nos primórdios da estatística, explorando uma anedota que dá o nome ao livro. O autor caminha por toda a Estatística, sem entrar nos detalhes, mas mostrando o que é, o contexto histórico dos pesquisadores da época, aplicações e etc. É um livro muito completo que fornece ao leitor um panorama do que é a Estatística. É um livro que tem uma leitura leve e agradável e é super indicado para quem nunca ouviu falar de estatística ou acredita que a área de resume a fazer gráficos e calcular médias! Você pode baixar uma amostra com a introdução nesse <a href="http://www.zahar.com.br/sites/default/files/arquivos//t1184.pdf">link</a> da editora Zahar. Esse é o livro de leitura mais agradável nessa lista. Uma pequena amostra: &#8220;<em>Tentei aqui algo um pouco menos ambicioso: descrever a revolução estatística na ciência do século XX por intermédio de algumas das pessoas (muitas delas ainda vivas) que nela estiveram envolvidas. Tratei muito superficialmente o trabalho que elas criaram, só para provar como suas descobertas individuais se encaixaram no quadro geral.</em>&#8220;</p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/03/effect_sizes.jpg" rel="attachment wp-att-892"><img class="wp-image-892 alignright" src="./../../../wp-content/uploads/2016/03/effect_sizes.jpg" alt="effect_sizes" width="158" height="224" srcset="./../../../wp-content/uploads/2016/03/effect_sizes.jpg 283w, ./../../../wp-content/uploads/2016/03/effect_sizes-212x300.jpg 212w" sizes="(max-width: 158px) 100vw, 158px" /></a>Nesse momento onde se discute bastante a respeito dos problemas com p-valor, p-hacking e afins eu achei esse livro muito interessante pois ele aborda esta e outras questões do ponto de vista das aplicações. O livro é leve, bem escrito e apresenta para o leitor a importância de entender o tamanho de efeito, como fazer uma análise de poder e como fazer meta análise. Não é um livro carregado de fórmulas e também não é um livro que poderia ser usado para um curso de análise de experimentos. Entretanto, para quem gostaria de entender o que é essa discussão toda sobre p-valor acho que esse livro pode ser muito útil.</p>
<h2 style="text-align: justify;">2. Data Science básico</h2>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/03/datascience_for_business.jpg" rel="attachment wp-att-860"><img class="wp-image-860 alignleft" src="./../../../wp-content/uploads/2016/03/datascience_for_business.jpg" alt="datascience_for_business" width="139" height="183" srcset="./../../../wp-content/uploads/2016/03/datascience_for_business.jpg 500w, ./../../../wp-content/uploads/2016/03/datascience_for_business-228x300.jpg 228w" sizes="(max-width: 139px) 100vw, 139px" /></a> Esse é aquele livro que eu sempre indico para alguém que é leigo, ouviu falar de big data, data mining ou coisa que o valha, e gostaria de saber do que se tratam estas coisas. É um livro pensando para esse público, muito bem escrito por profissionais da área, com ótimos exemplos e praticamente sem matemática. É um livro conceitual e portanto, apesar de apresentar diversos exemplos muito interessantes, o livro não tem códigos ou instruções de como implementar as análises em algum software. O objetivo do livro é responder ao leitor: o que é Data Science? O que eu posso fazer com isso? O que a minha empresa pode ganhar com isso?</p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/03/data_smart.jpg" rel="attachment wp-att-861"><img class=" wp-image-861 alignleft" src="./../../../wp-content/uploads/2016/03/data_smart.jpg" alt="data_smart" width="138" height="173" srcset="./../../../wp-content/uploads/2016/03/data_smart.jpg 260w, ./../../../wp-content/uploads/2016/03/data_smart-239x300.jpg 239w" sizes="(max-width: 138px) 100vw, 138px" /></a> Esse livro é muito interessante também, indo na mesma linha do Data Science for Business, mas mostrando outros exemplos interessantes e que se aprofunda um pouco mais nas técnicas e nos estudos de caso. Nesse livro o leitor é levado a analisar dados reais, mas utilizando simples planilhas eletrônicas como o Excel/LibreOffice Calc. Para não falar que são só planilhas, bem no final ele apresenta um exemplo com o R. No entanto, a ideia geral do livro é mostrar o que é uma análise de dados, o que você tem a ganhar com isso e como fazer isso no Excel. É bem legal e para um leitor que não é da área, mas quer dar um passo além, pondo data science em prática, esse é um livro muito bom.</p>
<h2 style="text-align: justify;">3. Linguagem R</h2>
<p style="text-align: justify;">Aqui eu vou mencionar livros fortemente relacionados ao ensino e uso do R. Alguns são introduções conceituais também mas que utilizam muito o R ao longo do livro.</p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/03/R_for_everyone.jpg" rel="attachment wp-att-863"><img class=" wp-image-863 alignleft" src="./../../../wp-content/uploads/2016/03/R_for_everyone.jpg" alt="R_for_everyone" width="160" height="203" srcset="./../../../wp-content/uploads/2016/03/R_for_everyone.jpg 260w, ./../../../wp-content/uploads/2016/03/R_for_everyone-237x300.jpg 237w" sizes="(max-width: 160px) 100vw, 160px" /></a> Pensando em um livro de introdução à linguagem R eu fiquei em dúvida entre este e um outro. Mas minha sugestão vai para este, uma vez que ele pode ser usado mais tarde como referência. É um livro muito bom, bem escrito e com bons exemplos. NO ENTANTO, existem diversos cursos online de introdução a programação em R que provavelmente eu indicaria ao invés de começar direto pelo livro. MAS cada pessoa aprende diferente, e alguém já versado em outras plataformas pode tirar vantagem da velocidade de aprender diretamente de um livro. Esse é um que eu gosto muito.</p>
<p><a href="./../../../wp-content/uploads/2016/03/data_manipulation_with_R.jpg" rel="attachment wp-att-871"><img class=" wp-image-871 alignleft" src="./../../../wp-content/uploads/2016/03/data_manipulation_with_R.jpg" alt="data_manipulation_with_R" width="157" height="238" srcset="./../../../wp-content/uploads/2016/03/data_manipulation_with_R.jpg 330w, ./../../../wp-content/uploads/2016/03/data_manipulation_with_R-198x300.jpg 198w" sizes="(max-width: 157px) 100vw, 157px" /></a></p>
<p style="text-align: justify;">Depois que alguém aprende o básico da linguagem R eu acredito que o grande salto de qualidade é entender exatamente como funcionam as principais estruturas de dados da linguagem, como o matrix, data.frame, list e etc. Além disso também acho muito importante entender como trabalhar com datas, como consultar bancos de dados relacionais, como alterar a estrutura de tabelas e etc. Enfim, uma série de conhecimentos com relação à manipulação de dados. Tudo isto está aqui nesse livro, que eu considero um dos melhores da série <a href="http://www.springer.com/series/6991">User R!</a>. Uma vez que se perde tanto tempo na etapa de preparação de dados, eu acredito que o conteúdo deste livro é essencial.</p>
<p><a href="./../../../wp-content/uploads/2016/03/The-Art-of-R-Programming-Matloff-Norman.jpg" rel="attachment wp-att-872"><img class="wp-image-872 alignright" src="./../../../wp-content/uploads/2016/03/The-Art-of-R-Programming-Matloff-Norman.jpg" alt="The-Art-of-R-Programming-Matloff-Norman" width="158" height="209" srcset="./../../../wp-content/uploads/2016/03/The-Art-of-R-Programming-Matloff-Norman.jpg 303w, ./../../../wp-content/uploads/2016/03/The-Art-of-R-Programming-Matloff-Norman-227x300.jpg 227w" sizes="(max-width: 158px) 100vw, 158px" /></a></p>
<p style="text-align: justify;">Além das técnicas que você vai aprender no livro anterior, o segundo próximo salto de qualidade que um usuário da linguagem R pode conseguir é aprender a como programar com eficiência no R. Quem já tem experiência com programação em outras linguagens costuma ter hábitos que no R podem deixar os scripts muito lentos. Esse livro é muito interessante neste aspecto, mostrando para o usuário porque isso é um problema e como você pode programar melhor. O livro também fala do processo de manipulação de strings e diversos outros tópicos super interessantes. Acho que é uma leitura obrigatória.</p>
<p><a href="./../../../wp-content/uploads/2016/03/ggplot2.jpg" rel="attachment wp-att-878"><img class="wp-image-878 alignleft" src="./../../../wp-content/uploads/2016/03/ggplot2.jpg" alt="ggplot2" width="159" height="239" srcset="./../../../wp-content/uploads/2016/03/ggplot2.jpg 333w, ./../../../wp-content/uploads/2016/03/ggplot2-200x300.jpg 200w" sizes="(max-width: 159px) 100vw, 159px" /></a></p>
<p style="text-align: justify;">Depois de aprender a usar bem a linguagem, o usuário provavelmente já deve estar versado no processo de gerar visualizações básicas com os gráficos do pacote base. ENTRETANTO, a maioria dos gráficos de altíssima qualidade que você vê por aí, gerados com R, são criados com o ggplot2. Eu acho ESSENCIAL aprender a utilizar este sistema gráfico. Assim, minha sugestão é correr os exemplos desse livro pelo menos uma vez, para entender a famosa &#8220;gramática dos gráficos&#8221; que o ggplot2 implementa. Fiquem atentos que a versão disponível desse livro para compra provavelmente é antiga, e uma nova versão atualizada estará sendo lançada em breve, agora em 2016.</p>
<p><a href="./../../../wp-content/uploads/2016/03/isl.jpg" rel="attachment wp-att-873"><img class="wp-image-873 alignright" src="./../../../wp-content/uploads/2016/03/isl.jpg" alt="isl" width="150" height="226" srcset="./../../../wp-content/uploads/2016/03/isl.jpg 1016w, ./../../../wp-content/uploads/2016/03/isl-199x300.jpg 199w, ./../../../wp-content/uploads/2016/03/isl-768x1157.jpg 768w, ./../../../wp-content/uploads/2016/03/isl-680x1024.jpg 680w" sizes="(max-width: 150px) 100vw, 150px" /></a></p>
<p style="text-align: justify;">Se você pretende trabalhar com Data Science e quer realmente entender como funcionam os algoritmos de machine learning minha sugestão é começar com este livro. É um livro de leitura tranquila, feito justamente para profissionais de outras áreas entenderem e aplicarem estes métodos e escrito por dois caras que eu sou fã. Esse livro é ótimo como uma introdução ao machine learning ou statistical learning onde todos os exemplos são implementados na linguagem R. Apesar de não ser um livro sobre a linguagem, é um livro ótimo caso você queira uma introdução ao assunto que utilize o R. Entretanto não é um daqueles livros de aplicações, é um livro teórico sobre o assunto, um livro que poderia ser utilizado em uma disciplina universitária por exemplo. Dois outros pontos fortes do livro são: 1) está disponível de graça <a href="http://www-bcf.usc.edu/~gareth/ISL/">aqui</a>. 2) Em janeiro os autores costumam oferecer um <a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about">MOOC</a> que é praticamente passar por todo este livro.</p>
<p><a href="./../../../wp-content/uploads/2016/03/ESLII.jpg" rel="attachment wp-att-874"><img class="wp-image-874 alignleft" src="./../../../wp-content/uploads/2016/03/ESLII.jpg" alt="SSS Hastie etal.qxd (Page 1)" width="149" height="224" srcset="./../../../wp-content/uploads/2016/03/ESLII.jpg 827w, ./../../../wp-content/uploads/2016/03/ESLII-200x300.jpg 200w, ./../../../wp-content/uploads/2016/03/ESLII-768x1154.jpg 768w, ./../../../wp-content/uploads/2016/03/ESLII-681x1024.jpg 681w" sizes="(max-width: 149px) 100vw, 149px" /></a></p>
<p style="text-align: justify;">Esse é o irmão mais velho do livro anterior. É uma verdadeira obra de referência na área, só que é um livro que apresenta o conteúdo em um nível que pode estar muito acima daquele estudante que está apenas começando na área de análise de dados. Os próprios autores afirmam que escreveram o &#8220;introduction&#8221; para remediar esse problema. Entretanto é um livro fantástico, super completo, cheio de ótimos exemplos tal que se você quiser um livro para entender todos os detalhes de machine learning (e tiver disposição para isso!) esse é o livro que você deve ter. O livro também é disponibilizado gratuitamente nesse <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">link</a>, mas o livro impresso é de uma qualidade impressionante, vale muito a pena.</p>
<p><a href="./../../../wp-content/uploads/2016/03/applied_predictive_modelling.jpg" rel="attachment wp-att-875"><img class=" wp-image-875 alignleft" src="./../../../wp-content/uploads/2016/03/applied_predictive_modelling.jpg" alt="applied_predictive_modelling" width="147" height="222" /></a></p>
<p style="text-align: justify;">Esse é o livro irmão do &#8220;Introduction of Statistical Learning&#8221;, como os próprio autores afirmam. O livro apresenta exemplos reais de análises utilizando as técnicas apresentadas no &#8220;introduction&#8221;. Você vai ver exemplos de classificação, avaliação de modelos, regressão, etc. Os dados são dados reais utilizados em pesquisas dos autores. O livro é muito interessante para ver como se faz data science na realidade. Cheio de exemplos super interessantes em áreas como quimiometria, detecção de fraude, segmentação de clientes e etc. Os autores abordam problemas como: seleção de atributos, problemas com classe desbalanceada, preenchimento de dados faltantes e etc. O livro portanto tem um enfoque nos aspectos práticos da modelagem e deixa a teoria sobre os modelos utilizados para outros livros. Os autores deste livro são os mesmos autores do pacote <a href="http://caret.r-forge.r-project.org/">caret</a> que é tão utilizado pela comunidade R para automatizar tarefas de modelagem. Eles utilizam o pacote extensivamente ao longo do livro.</p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/03/handook_data_mining.jpg" rel="attachment wp-att-890"><img class="wp-image-890 alignleft" src="./../../../wp-content/uploads/2016/03/handook_data_mining.jpg" alt="handook_data_mining" width="151" height="185" srcset="./../../../wp-content/uploads/2016/03/handook_data_mining.jpg 300w, ./../../../wp-content/uploads/2016/03/handook_data_mining-244x300.jpg 244w" sizes="(max-width: 151px) 100vw, 151px" /></a> Por fim, eu gostaria de adicionar esse livro a lista porque é um livro super interessante para discutir em linhas gerais o processo de data mining. Ele é um livro mais na linha do livros clássicos de data mining, mas que conta com muitas aplicações. Neste livro são apresentadas análises práticas de problemas de predição de churn, de fraude, segmentação de clientes, previsão de risco e etc. Só pelos exemplos das aplicações já vale a pena. As aplicações não utilizam o R, mas utilizam SAS, Stata, SPSS dentre outros. É uma boa fonte também para quem quer ver como se faz esse tipo de análise em outros softwares.</p>
<h2 style="text-align: justify;">Conclusão</h2>
<p style="text-align: justify;">Está longe de ser uma lista exaustiva, muitos bons títulos que eu não conheço devem ter ficado de fora, mas são todos livros que fizeram muito a diferença para mim. Também me restringi aos livros que eu realmente li e usei, e muitos aí eu uso como referência até hoje. Eu teria outras sugestões para livros sobre experimentos, livros sobre análise de survey, séries temporais e etc. Entretanto eu coloquei mais uma bibliografia básica sobre o que costuma ser abordado em currículos de cursos de Data Science que eu vejo por aí.</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2016/03/12/livros-recomendados-data-science/feed/index.html</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
		<item>
		<title>Regras de associação: vendas cruzadas e recomendação</title>
		<link>./../../../2016/03/01/regras-de-associacao-vendas-cruzadas-e-recomendacao/index.html</link>
		<comments>./../../../2016/03/01/regras-de-associacao-vendas-cruzadas-e-recomendacao/index.html#comments</comments>
		<pubDate>Wed, 02 Mar 2016 00:00:10 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Data Mining]]></category>
		<category><![CDATA[Educação]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">./../../../index.html?p=841</guid>
		<description><![CDATA[Caros leitores, fizemos um novo hangout na semana passada, desta vez sobre regras de associação. Vocês podem conferir aqui o vídeo: Para resumir, no vídeo falamos um pouco sobre o que são as regras de associação, as aplicações em vendas cruzadas e recomendação e foi apresentado também um exemplo prático da famosa &#8220;market basket analysis&#8221; ou análise de cestas de mercado. O material usado na apresentação, com os slides em PDF e os códigos, está...]]></description>
				<content:encoded><![CDATA[<p>Caros leitores, fizemos um novo hangout na semana passada, desta vez sobre regras de associação. Vocês podem conferir aqui o vídeo:</p>
<p><iframe width="640" height="360" src="https://www.youtube.com/embed/yg9DRPWi524?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<p style="text-align: justify;">Para resumir, no vídeo falamos um pouco sobre o que são as regras de associação, as aplicações em vendas cruzadas e recomendação e foi apresentado também um exemplo prático da famosa &#8220;market basket analysis&#8221; ou análise de cestas de mercado.</p>
<p style="text-align: justify;">O material usado na apresentação, com os slides em PDF e os códigos, está disponível no <a href="https://github.com/flaviobarros/lombz_association_rules" target="_blank">github</a>. E também gostaria de deixar como sugestão a leitura dos artigos sobre como implementar regras de associação com <a href="https://pt.wikipedia.org/wiki/MapReduce" target="_blank">mapreduce</a> (em Big Data) e o outro artigo sobre como o Youtube usou regras de associação no seu sistema de recomendação de vídeos. Aqui vão os links:</p>
<ul>
<li style="text-align: justify;"><a href="http://www.inf.unibz.it/~ricci/ISR/papers/p293-davidson.pdf" target="_blank">The YouTube Video Recommendation System</a></li>
<li style="text-align: justify;"><a href="http://worldcomp-proceedings.com/proc/p2012/PDP7948.pdf" target="_blank">Apriori-Map/Reduce Algorithm</a></li>
</ul>
<p>Por favor, não deixe de curtir nossa página no Facebook!</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2016/03/01/regras-de-associacao-vendas-cruzadas-e-recomendacao/feed/index.html</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Cluster &#8211; Segmentação de Clientes</title>
		<link>./../../../2016/02/22/cluster-segmentacao-de-clientes/index.html</link>
		<comments>./../../../2016/02/22/cluster-segmentacao-de-clientes/index.html#respond</comments>
		<pubDate>Mon, 22 Feb 2016 07:46:50 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Data Mining]]></category>
		<category><![CDATA[Educação]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">./../../../index.html?p=830</guid>
		<description><![CDATA[OBS: Caros visitantes, curtam a  página do R Mining no Facebook, aqui ao lado! Agradeço muito. Caros leitores do blog, por conta de diversos fatores eu só estou conseguindo postar agora, pela primeira vez esse ano, em fevereiro. Enfim, demorou, mas eu tenho algo que eu acho que pode ser interessante. Um grupo de amigos, arquitetos de soluções em grandes empresas de São Paulo, está organizando alguns hangouts sobre Big Data. Você pode assistir os...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;"><strong>OBS: Caros visitantes, curtam a  página do R Mining no Facebook, aqui ao lado! Agradeço muito.</strong></p>
<p style="text-align: justify;">Caros leitores do blog, por conta de diversos fatores eu só estou conseguindo postar agora, pela primeira vez esse ano, em fevereiro. Enfim, demorou, mas eu tenho algo que eu acho que pode ser interessante. Um grupo de amigos, arquitetos de soluções em grandes empresas de São Paulo, está organizando alguns hangouts sobre Big Data. Você pode assistir os hangouts que estão disponíveis até o momento nesse <a href="https://www.youtube.com/playlist?list=PLACuKp_68Ygn6UxGOwJeZx7T9nD5NZiFt" target="_blank">playlist</a>, ou mesmo ver um por um aqui:</p>
<ul>
<li>BIG DATA &#8211; Buscando as respostas onde elas estão.</li>
</ul>
<p><iframe width="640" height="360" src="https://www.youtube.com/embed/efYAIH5dprc?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<ul>
<li>Falando sobre Arquitetura para Big Data.</li>
</ul>
<p><iframe width="640" height="360" src="https://www.youtube.com/embed/smloHdrdbdY?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<ul>
<li>Cluster &#8211; Segmentação de Clientes</li>
</ul>
<p><iframe width="640" height="360" src="https://www.youtube.com/embed/vkOObS6N7ZM?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<p style="text-align: justify;">Eu tive a oportunidade de ser  comentarista em dois deles.</p>
<p style="text-align: justify;">E o que é esse post então? Bom, como de repente alguém poderia querer ver os códigos utilizados no terceiro vídeo, e até alguns exemplos suplementares, eu vou apresentar aqui o link dos códigos no <a href="https://github.com/flaviobarros/lombz_cluster">Github</a> e também vou destacar o que eu acho interessante sobre a análise.</p>
<ul style="text-align: justify;">
<li style="text-align: justify;">Essa é uma análise simples e o conjunto de dados é bem pequeno;</li>
<li style="text-align: justify;">A ideia é apresentar o conceito de cluster;</li>
<li style="text-align: justify;">Aplicação com segmentação de clientes;</li>
<li style="text-align: justify;">Descobrir o número de clusters pelo dendograma;</li>
<li style="text-align: justify;">Descobrir o número de clusters pelo método do cotovelo;</li>
<li style="text-align: justify;">Após o agrupamento a análise dos grupos provê insights;</li>
</ul>
<p style="text-align: justify;">Aqui naturalmente eu não abordei diversos tópicos no que diz respeito a aplicação da metodologia de clusters, mas como uma introdução acho que foi interessante. De resto, não vou colocar mais detalhes, como eu faço regularmente nas análises aqui, pois já temos os códigos e também o vídeo sobre a discussão.</p>
<p style="text-align: justify;">
]]></content:encoded>
			<wfw:commentRss>./../../../2016/02/22/cluster-segmentacao-de-clientes/feed/index.html</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Reconhecimento de dígitos escritos a mão – PARTE 2</title>
		<link>./../../../2015/09/14/reconhecimento-de-digitos-escritos-a-mao-parte-2/index.html</link>
		<comments>./../../../2015/09/14/reconhecimento-de-digitos-escritos-a-mao-parte-2/index.html#comments</comments>
		<pubDate>Mon, 14 Sep 2015 12:00:02 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Aprendizado de Máquina]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[Preparação de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">http://www.flaviobarros.net/?p=746</guid>
		<description><![CDATA[Na Parte 1 desse post (que já publiquei faz um tempão!) eu fiz uma classificação de imagens de dígitos escritos mão usando o k-nn (algoritmo dos vizinhos mais próximos) usando as informações das imagens sem nenhum tipo de tratamento, isto é, sem nenhum método de preparação. Como foi mostrado, o k-nn só foi capaz de classificar razoavelmente bem com com k = 1 e conseguiu uma acurácia de apenas 78%, algo muito distante do que ainda...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">Na <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://www.flaviobarros.net/2014/12/22/reconhecimento-de-digitos-escritos-mao-parte-1/">Parte 1</a></span> desse post (que já publiquei faz um tempão!) eu fiz uma classificação de imagens de dígitos escritos mão usando o <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nn</a> (algoritmo dos vizinhos mais próximos) usando as informações das imagens sem nenhum tipo de tratamento, isto é, sem nenhum método de preparação. Como foi mostrado, o <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nn</a> só foi capaz de classificar razoavelmente bem com com k = 1 e conseguiu uma acurácia de apenas 78%, algo muito distante do que ainda pode ser conseguido.</p>
<p style="text-align: justify;">Assim, nesta segunda parte, vou explorar outras alternativas além do <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nn</a>, e mais do que isso, tentar aplicar algum método de preparação para melhorar a performance dos algoritmos. Aqui vou repetir o procedimento de leitura do outro post, para facilitar a reprodução dessa análise.</p>
<p style="text-align: justify;">OBS: Leia a <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://www.flaviobarros.net/2014/12/22/reconhecimento-de-digitos-escritos-mao-parte-1/">Parte 1</a></span> para entender melhor o problema!</p>
<h3 style="text-align: justify;">1. LEITURA</h3>
<p style="text-align: justify;">Os dados do problema são imagens do tipo <a href="http://en.wikipedia.org/wiki/Netpbm_format"><span style="color: #0000ff;">PGM</span></a>, com 64 x 64 pixels por imagem, onde cada pixel tem valor 1 ou 0, indicando se o pixel é preto ou branco. Cada imagem tem um nome no formato X_ yyy.BMP.inv.pgm, onde o X representa o dígito desenhado na imagem. Os dados estão divididos em um conjunto de treino e um conjunto de teste e podem ser baixados nos seguintes links: <a href="http://www.flaviobarros.net/wp-content/uploads/2014/12/teste.zip">teste</a>  e <a href="http://www.flaviobarros.net/wp-content/uploads/2014/12/treino.zip">treino</a></p>
<p style="text-align: justify;">Assim a primeira parte do problema é efetuar a leitura dos dados. Para isso me utilizarei do pacote <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://cran.r-project.org/web/packages/pixmap/index.html">pixmap</a></span> com o qual é possível ler e manipular imagens PGM. A seguir, o processo de leitura das imagens e a criação de um vetor com as respostas, isto é, com o número do dígito que está escrito na imagem.</p>
<pre class="lang:r decode:true">## Carregando o pacote pixmap
library(pixmap)

## Definindo o diretório com as imagens de treino
path_treino &lt;- '/sua/pasta/treino/'

## Define como diretório de trabalho
setwd(path_treino)

## Lê os nomes dos arquivos
files &lt;- dir()

## Retira as classes dos nomes dos arquivos
classes &lt;- as.factor(substring(files,first=1,last=1))

## Cria data.frame para armazenar dados de treino
treino &lt;- as.data.frame(matrix(rep(0,length(files)*64*64), nrow=length(files)))

## Efetua leitura dos dados de treino
for (i in 1:length(files)) {

  ## Lendo as imagens
  x &lt;- read.pnm(files[i])

  ## No slot 'grey' está a matriz de pixels que é retirada e vetorizada
  treino[i,] &lt;- as.vector(x@grey, mode='integer')
}

## Define como diretório de trabalho o local das imagens para teste
path_teste &lt;- '/sua/pasta/teste/'

## Diretório de trabalho
setwd(path_teste)

## Lê os nomes dos arquivos
files &lt;- dir()

## Classes
predic &lt;- as.factor(substring(files,first=1,last=1))

## Cria data.frame para armazenar conjunto de teste
teste &lt;- as.data.frame(matrix(rep(0,length(files)*64*64), nrow=length(files)))

## Leitura do conjunto de teste
for (i in 1:length(files)) {
  x &lt;- read.pnm(files[i])
  teste[i,] &lt;- as.vector(x@grey, mode='integer')
}</pre>
<p style="text-align: justify;">É importante observar que a matriz de pixels fica armazenada no slot @grey, e que após a leitura é transformada em um vetor, tal que o data.frame final fica com 64&#215;64 colunas e 1949 linhas (o total de imagens). O conjunto de  teste tem somente 50 imagens, logo o data.frame vai ficar com 64&#215;64 colunas e somente 50 linhas. Em suma, cada coluna é um píxel e cada linha uma das diferentes imagens.</p>
<h2 style="text-align: justify;">Aplicação do PCA</h2>
<p style="text-align: justify;">O primeiro procedimento que vou utilizar aqui para tentar melhorar a performance da classificação, é usar o método de <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://pt.wikipedia.org/wiki/An%C3%A1lise_de_Componentes_Principais">Análise de Componente Principais</a></span>, para fazer a rotação do sistema de coordenadas e então ficar somente com uma parte das componentes principais. No fim das contas, o que estaremos fazendo é uma redução do número de atributos pois preservando somente parte das componentes principais eu estarei reduzindo o número de colunas da matriz de dados.</p>
<p style="text-align: justify;">A ideia por trás desse procedimento é que nem todos os atributos (que nesse  caso são pixels) tem a mesma importância para entender o que está escrito na imagem. Pense que pixels que estão próximos da borda podem ter uma importância menor ao passo que pixels mais ao centro devem ser bem mais importantes. Veja que se tivéssemos 10 atributos (ou variáveis), digamos X1 X2 X3 &#8230; X10, então pela rotação do sistema obteríamos PC1 PC2 PC3 &#8230; PC10. A diferença é que as componentes principais, que serão utilizadas no lugar dos atributos originais, são ordenadas de acordo com a variância capturada, isto  é, a PC1 captura maior variância que a PC2 e assim por diante. Dessa forma poderemos escolher as primeiras que capturam a maior variância desejada, reduzindo assim o número de atributos que entram no modelo.</p>
<p style="text-align: justify;">Um detalhe importante da aplicação desse método é que após encontrar as componentes principais no conjunto de treino DEVEMOS APLICAR A MESMA ROTAÇÃO no conjunto de teste. Essa matriz de rotação DEVE SER A MESMA OBTIDA NO CONJUNTO DE TREINO, uma vez que eu não quero usar um método de redução de atributos que tenha utilizado qualquer informação do conjunto de testes. Isso é importante para garantir que a acurácia global inferida será o mais próximo possível do acurácia do modelo em novas imagens.</p>
<h3 style="text-align: justify;">PCA com prcomp</h3>
<p style="text-align: justify;">Para aplicar o PCA agora vou utilizar a função do pacote base <a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html">prcomp</a>. Com ela vamos obter as componentes principais, no caso o mesmo número de colunas do conjunto de dados original, e também o desvio padrão de cada uma delas. Com essas duas informações poderemos avaliar a variância acumulada das componentes principais e escolher somente as primeiras que contemplem, vamos dizer, 90% da variância nesse conjunto de dados.</p>
<pre class="lang:r decode:true">############ APlicação do PCA ####################################
## Obtendo componentes principais
pc &lt;- prcomp(treino)
treino_x &lt;- pc$x

## Obtendo as variâncias acumuladas
vars = pc$sdev^2

## Calculando a proporção de variância 
props = vars/sum(vars)

## Obtendo as variâncias acumulada
varAcum = cumsum(props)

## Obtendo o número de componentes que capturam pelo menos 90% da variância.
which.min(varAcum &lt; 0.9)

## Aplicando a rotação nos dados de teste
test.p &lt;- predict(pc, newdata = teste)</pre>
<p style="text-align: justify;">Com 341 componentes principais capturamos 90% da variância. Isto é uma redução agressiva no número de atributos, uma vez que tínhamos mais de 1900 pixels. Agora estamos prontos para aplicar o método k-nn novamente, mas desta vez usando as 400 componentes principais como input (valor aproximado). Esperamos uma melhora.</p>
<h2 style="text-align: justify;">K-nn com PCA</h2>
<pre class="lang:r decode:true">## Treinando o knn com as 341 variáveis
predito &lt;- knn(train=pc$x[,1:400], test=test.p[,1:400], cl=classes, k=1, prob=T)

result &lt;- data.frame(cbind(predic, predito, acerto = predic==predito))

sum(result$acerto)/nrow(result)</pre>
<p style="text-align: justify;">Com k = 1 obtivemos uma performance de 84% de acurácia que já é uma melhora. Há que se notar também que não adianta muito colocar mais componentes e que também outros valores de k parecem não trazer grandes melhoras. Talvez seja a hora de partir para outros tipos de modelos a partir daqui. A seguir o gráfico do desempenho do k-nn ao longo dos valores de k com a inclusão de novas componentes.</p>
<p style="text-align: justify;"><a href="http://www.flaviobarros.net/wp-content/uploads/2015/09/valores_dim.png"><img class="aligncenter size-full wp-image-749" src="http://www.flaviobarros.net/wp-content/uploads/2015/09/valores_dim.png" alt="valores_dim" width="480" height="480" srcset="./../../../wp-content/uploads/2015/09/valores_dim.png 480w, ./../../../wp-content/uploads/2015/09/valores_dim-150x150.png 150w, ./../../../wp-content/uploads/2015/09/valores_dim-300x300.png 300w, ./../../../wp-content/uploads/2015/09/valores_dim-65x65.png 65w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<h2 style="text-align: justify;">Árvore de decisão com PCA</h2>
<p style="text-align: justify;">Parece que não vamos muito mais longe com o k-nn, assim vamos tentar outros métodos como as árvores de decisão. Aqui vou usar a implementação do algoritmo <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">CART</a> no pacote <a href="https://cran.r-project.org/web/packages/rpart/index.html">rpart</a> do R. Vou usar como entrada as 400 componentes principais.</p>
<pre class="lang:r decode:true ">## Data frame de treino e teste
treino_arvore = as.data.frame(cbind(treino_pc[,1:544]))
treino_arvore$classes = classes

teste_arvore = as.data.frame(teste_pc[,1:544])

## Teste com árvore de decisão
library(rpart)

## Criando uma árvore
arvore &lt;- rpart(classes ~ ., data = treino_arvore)

## Fazendo a previsão no conjunto de teste
predito &lt;- predict(object = arvore, newdata = teste_arvore, type = 'class')

## Resultado
result &lt;- data.frame(cbind(predic, predito, acerto = predic==predito))

## Cálculo da taxa de acerto
sum(result$acerto)/nrow(result)</pre>
<p>O resultado foi inferior ao k-nn, com somente 72% de acurácia global.</p>
<p>A seguir vou apresentar ainda mais algumas tentativas com uma regressão linear, regressão logística multinomial e o randomForest.</p>
<h2>Outros modelos com PCA</h2>
<pre class="lang:r decode:true ">## Teste com logística multinomial
library(nnet)

## Ajustando um modelo
multinomial &lt;- multinom(classes ~ ., data = treino_arvore, MaxNWts = 5460)

## Fazendo a previsão no conjunto de teste
predito &lt;- predict(object = multinomial, newdata = teste_arvore, type = 'class')

## Resultado
result &lt;- data.frame(cbind(predic, predito, acerto = predic==predito))

## Cálculo da taxa de acerto
sum(result$acerto)/nrow(result)

#################################################################
## Teste com randomForest
library(randomForest)

## Ajustando um modelo
rf &lt;- randomForest(classes ~ ., data = treino_arvore)

## Fazendo a previsão no conjunto de teste
predito &lt;- predict(object = rf, newdata = teste_arvore, type = 'class')

## Resultado
result &lt;- data.frame(cbind(predic, predito, acerto = predic==predito))

## Cálculo da taxa de acerto
sum(result$acerto)/nrow(result)

#################################################################
## Teste com regressão linear
## Data frame de treino e teste
treino_arvore = as.data.frame(cbind(treino_pc[,1:544]))
treino_arvore$classes = classes

teste_arvore = as.data.frame(teste_pc[,1:544])

## Criando uma árvore
reg &lt;- lm(classes ~ ., data = treino_arvore)

## Fazendo a previsão no conjunto de teste
predito &lt;- predict(object = fit, newdata = teste_arvore)
predito &lt;- round(predito, 0)

## Resultado
result &lt;- data.frame(cbind(predic, predito, acerto = predic==predito))

## Cálculo da taxa de acerto
sum(result$acerto)/nrow(result)</pre>
<p>Com desempenhos inferiores ou iguais ao k-nn.</p>
<h2>Resumos dos resultados</h2>
<ul>
<li>k-nn com k = 1: 84%</li>
<li>Regressão Linear: 14%</li>
<li>Regressão Logística Multinomial: 64%</li>
<li>Árvore de Decisão: 34%</li>
<li>RandomForest: 72%</li>
</ul>
<h2>Conclusão</h2>
<p style="text-align: justify;">Por enquanto parece que não conseguimos passar do teto de 84% com o k-nn. Isso pode acontecer por diversos motivos, mas alguns possíveis responsáveis por ainda não alcançarmos os melhores resultados são:</p>
<ol style="text-align: justify;">
<li>No conjunto original teríamos mais de 60 mil imagens para treinar os algoritmos;</li>
<li>O conjunto de teste é muito pequeno logo há muita variabilidade na estimativa de erro;</li>
<li>Usamos parâmetros default em todos os modelos;</li>
</ol>
<p style="text-align: justify;">No próximo post vamos tentar encontrar os melhores hiperparâmetros para alguns modelos, tentar avaliar melhor o desempenho dos algoritmos, tentar novos algoritmos e automatizar tudo com o <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://caret.r-forge.r-project.org/">caret</a></span>.</p>
<p style="text-align: justify;">Gostou do artigo? Curta nossa página no Facebook!</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2015/09/14/reconhecimento-de-digitos-escritos-a-mao-parte-2/feed/index.html</wfw:commentRss>
		<slash:comments>7</slash:comments>
		</item>
		<item>
		<title>Curso &#8220;As Ferramentas do Cientista de Dados&#8221; do Coursera</title>
		<link>./../../../2015/09/10/curso-as-ferramentas-do-cientista-de-dados-do-coursera/index.html</link>
		<comments>./../../../2015/09/10/curso-as-ferramentas-do-cientista-de-dados-do-coursera/index.html#comments</comments>
		<pubDate>Fri, 11 Sep 2015 00:24:53 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Educação]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Git & Github]]></category>
		<category><![CDATA[MOOCS]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">http://www.flaviobarros.net/?p=741</guid>
		<description><![CDATA[Hoje eu vou fazer uma pequena resenha sobre o curso &#8220;As Ferramentas do Cientista de Dados&#8221; oferecido pelo Coursera, com o objetivo de avaliar o curso e direcionar os leitores sobre a escolha de fazê-lo ou não. Sobre o que é o curso? Inicialmente eu gostaria de destacar que esse é o curso inicial da especialização em Data Science do Coursera, oferecida em parceria com professores da Johns Hopkins University. A especialização compreende 10 cursos...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">Hoje eu vou fazer uma pequena resenha sobre o curso &#8220;<span style="color: #0000ff;"><a style="color: #0000ff;" href="https://www.coursera.org/course/datascitoolbox">As Ferramentas do Cientista de Dados</a></span>&#8221; oferecido pelo Coursera, com o objetivo de avaliar o curso e direcionar os leitores sobre a escolha de fazê-lo ou não.</p>
<h2 style="text-align: justify;">Sobre o que é o curso?</h2>
<p style="text-align: justify;">Inicialmente eu gostaria de destacar que esse é o curso inicial da especialização em <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://www.coursera.org/specializations/jhudatascience?utm_medium=courseDescripTop">Data Science do Coursera</a></span>, oferecida em parceria com professores da Johns Hopkins University. A especialização compreende 10 cursos que em tese capacitam o aluno como um cientista de dados júnior.</p>
<p style="text-align: justify;">Esse curso em especial atua como uma forma de preparação inicial, introduzindo as ideias da especialização como um todo e fornecendo aos alunos um direcionamento com relação as ferramentas que ele vai utilizar durante os 9 cursos seguintes. Em resumo, ele oferece:</p>
<ul>
<li style="text-align: justify;">Introdução sobre todos os outros 9 cursos</li>
<li style="text-align: justify;">Breve introdução ao <a href="https://www.r-project.org/">R</a> &amp; <a href="https://www.rstudio.com/">RStudio</a></li>
<li style="text-align: justify;">Como instalar pacotes no <a href="https://www.r-project.org/">R</a></li>
<li style="text-align: justify;">Introdução ao <a href="http://rmarkdown.rstudio.com/">RMarkdown</a></li>
<li style="text-align: justify;"><a href="https://git-scm.com/book/pt-br/v1/Primeiros-passos-No%C3%A7%C3%B5es-B%C3%A1sicas-de-Git">Git</a> e <a href="https://github.com/">Github</a></li>
<li style="text-align: justify;">Introdução ao que é Ciência de Dados e Big Data</li>
</ul>
<h2>Quanto tempo leva?</h2>
<p style="text-align: justify;">O curso é previsto para 3 semanas, mas de fato com um mínimo de dedicação é possível realizar TODAS AS TAREFAS EM 1 HORA. Não estou brincando, realmente em uma hora você pode terminar tudo, principalmente se você já usou um <a href="http://git-scm.com/">git</a> ou <a href="https://github.com/">github</a> pelo menos uma vez na vida. Eu me inscrevi para avaliar o curso e fiz tudo em uma hora.</p>
<h2 style="text-align: justify;">Vale a pena?</h2>
<p style="text-align: justify;">Essa é uma questão difícil de responder, mas atualmente eu diria que não. Por quê? Porque o curso isoladamente não significa nada. Se o aluno pretende aprender a usar o <a href="http://git-scm.com/">git</a> e <a href="https://github.com/">github</a> existem recursos gratuitos muito melhores na internet. Além disso a introdução que ele faz dos outros cursos, e mesmo alguns conteúdos apresentados, bem poderiam fazer parte dos outros 9 cursos da especialização. Enfim, só faz sentido fazer esse curso se você planeja terminar a especialização inteira, uma vez que você é obrigado a concluir os 10 cursos para poder participar do <a href="https://www.coursera.org/course/dsscapstone">projeto final</a>.</p>
<h2 style="text-align: justify;">Nem a parte sobre Git vale a pena?</h2>
<p style="text-align: justify;">Como eu disse eu acredito que esse curso poderia ser canibalizado pelos outros, e na minha opinião, não havia a necessidade de criar um curso separado só para isso. Ainda assim posso dizer que a introdução ao Git e Github é razoável, pensando em um usuário que nunca viu Git e não sabe o que é.</p>
<h2 style="text-align: justify;">Conclusão</h2>
<p style="text-align: justify;">Se você pretende fazer a especialização vai ter que fazer esse curso, caso contrário existem materiais melhores, gratuitos, onde você pode ter uma boa ideia e uma boa introdução a todos os tópicos tratados nesse curso.</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2015/09/10/curso-as-ferramentas-do-cientista-de-dados-do-coursera/feed/index.html</wfw:commentRss>
		<slash:comments>7</slash:comments>
		</item>
		<item>
		<title>Preparação de dados &#8211; Parte 2</title>
		<link>./../../../2015/09/07/preparacao-de-dados-parte-2/index.html</link>
		<comments>./../../../2015/09/07/preparacao-de-dados-parte-2/index.html#respond</comments>
		<pubDate>Mon, 07 Sep 2015 13:00:37 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[Preparação de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">http://www.flaviobarros.net/?p=730</guid>
		<description><![CDATA[Neste post eu vou falar sobre como trabalhar com GRANDES ARQUIVOS DE TEXTO em chunks no R. Esse pode ser um problema complicado e que pode aparecer na vida do analista trabalhando com arquivos de log por exemplo. Antes de continuar o post gostaria de salientar que estou utilizando o termo chunk para designar um pedaço do arquivo de texto, isto é, estou dizendo que vamos trabalhar com grandes arquivos de texto, pedaço por pedaço. Mas por que...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">Neste post eu vou falar sobre como trabalhar com GRANDES ARQUIVOS DE TEXTO em <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://www.wordreference.com/enpt/chunk">chunks</a></span> no R. Esse pode ser um problema complicado e que pode aparecer na vida do analista trabalhando com <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://en.wikipedia.org/wiki/Logfile">arquivos de log</a></span> por exemplo. Antes de continuar o post gostaria de salientar que estou utilizando o termo <em>chunk </em>para designar um pedaço do arquivo de texto, isto é, estou dizendo que vamos trabalhar com grandes arquivos de texto, pedaço por pedaço.</p>
<h2>Mas por que chunks?</h2>
<p style="text-align: justify;">Bom, se você já teve oportunidade de trabalhar com grandes arquivos de dados no R você já deve ter percebido que dependendo do tamanho do arquivo o simples procedimento de leitura pode demorar muito tempo. Isso ocorre porque o R trabalha com arquivos de dados diretamente na memória RAM tal que arquivos gigantescos ou podem ocupar toda a memória RAM do seu computador, inviabilizando a leitura, ou mesmo podem até ser lidos mas deixarem o computador extremamente lento tomando muito tempo da análise.</p>
<p style="text-align: justify;">Para mostrar um exemplo prático de como trabalhar com este tipo de arquivo no R vamos utilizar um conjunto de dados muito interessante com informações de vôos, o <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://stat-computing.org/dataexpo/2009/the-data.html">Airlines</a></span>. Vamos trabalhar somente com as informações relativas a 1988, mas que sozinhas já são suficiente para dar uma tremenda dor de cabeça. Para trabalhar com esses dados eu vou usar o pacote <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://cran.r-project.org/web/packages/iterators/index.html">iterators</a></span>, um pacote que permite ler um arquivo linha por linha, ou chunk por chunk, mas sem ter que carregar o arquivo inteiro na memória. Para você ter uma ideia de como isso funciona tente rodar esse código:</p>
<pre class="lang:r decode:true">## Instalando e carregando o pacote
install.packages('iterators')
library(iterators)

## Efetuando a conexão de leitura do arquivo, direto do formato bz2 (uma espécie de zip).
con &lt;- bzfile('1988.csv.bz2', 'r')</pre>
<p style="text-align: justify;">Preste atenção que o objeto <strong>con</strong> armazena somente uma CONEXÃO. EU não estou lendo o arquivo ainda e portanto não o estou carregando na memória. Esse tipo de conexão é parecida com a que você poderia criar em um <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://pt.wikipedia.org/wiki/Sistema_de_gerenciamento_de_banco_de_dados">sistema de gerenciamento de banco de dados</a></span> antes de fazer a primeira consulta em SQL.</p>
<p style="text-align: justify;">OK, agora que temos a conexão vamos criar um iterator (ou iterador):</p>
<pre class="lang:r decode:true">## Cria um iterador que lê uma linha do arquivo por vez (n = 1)
it &lt;- ireadLines(con, n=1)

## Comando para ir retornando linha a linha
nextElem(it)
nextElem(it)</pre>
<p style="text-align: justify;">Como você pode ver você está imprimindo no terminal linha por linha do arquivo. Assim, se você quiser trabalhar linha por linha, ou mesmo chunk por chunk (n &gt; 1) você pode ir carregando pequenos pedaços do arquivo que não vão ocupar a memória inteira.</p>
<h2 style="text-align: justify;">Mas como saber qual é a última linha?</h2>
<p style="text-align: justify;">Como estamos lendo linha a linha não temos como saber de antemão quantas linhas devemos ler. Assim a solução para o problema é tentar pegar o erro que ocorre quando chegamos na última linha do arquivo e pedimos mais uma com o nextElem(it). Quando fazemos isso a função retorna um erro, o qual podemos &#8220;transformar&#8221; em um FALSE do R e usar esse sinal em um programa para encerrar o trabalho.</p>
<pre class="lang:r decode:true">tryCatch(expr=nextElem(it), error=function(e) return(FALSE))</pre>
<p>esse comando retorna um  FALSE quando chega no fim do arquivo. Esse é um truque muito útil na preparação com grandes arquivos de texto.</p>
<h2>Exemplo do Stackoverflow</h2>
<p style="text-align: justify;">Há um tempo atrás surgiu uma pergunta no stackoverflow justamente sobre grandes arquivos de texto. Você encontrar a pergunta aqui: <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://pt.stackoverflow.com/questions/35469/pre-processar-grande-arquivo-de-texto-txt-substituir-por/35645#35645" target="_blank">Pre-processar grande arquivo de texto</a></span>. Veja que há também uma solução interessante do Carlos Cinelli (do <a href="http://analisereal.com/" target="_blank">analisereal.com</a>) que não faz uso do iterators. É uma solução que usa o seguinte método: lê 4000 linhas por vez e pula as linhas já lidas anteriormente. Eu acredito que a estratégia com iterators pode ser melhor pois você só vai varrer o arquivo uma vez, isto é, na solução com iterator você tem um apontador para a linha que parou e pode continuar a partir dali, até chegar no final do arquivo. Na solução usando o read.csv() ou read.table() em cada iteração o arquivo é lido novamente e todas as linhas já lidas são puladas.</p>
<pre class="lang:r decode:true ">## Carregando o pacote
library(iterators)

## Defindo a função que troca vírgula por ponto
change_dot &lt;- function(file, saida='teste.txt', chunk=1) {

  ## Cria conexão com arquivo a ser lido
  con1 &lt;- file(file, 'r')

  ## Cria conexão onde será escrito o novo arquivo
  con2 &lt;- file(saida, open = 'w')
  linha &lt;- 0

  ## Criando o iterador
  it &lt;- ireadLines(con1, n=chunk)

  ## Lendo a primeira linha e escrevendo no outro arquivo
  out &lt;- tryCatch(expr=write(x = gsub(pattern = ',', replacement = '.', x = nextElem(it)), con2), 
                   error=function(e) e)

  ## Observe o uso do tryCatch() para saber o fim do arquivo
  while(!any(class(out) == "error")) {
    linha = linha + 1
    print(paste('Escrita linha ', linha))
    out &lt;- tryCatch(expr=write(x = gsub(pattern = ',', replacement = '.', x = nextElem(it)), con2, append = T), 
                  error=function(e) e)
  }
}</pre>
<p>Nesse caso em específico o tempo total para a realização da tarefa foi de aproximadamente 6 segundos.</p>
<p>Gostou do artigo ou achou a dica útil?! Curta nossa página no Facebook!</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2015/09/07/preparacao-de-dados-parte-2/feed/index.html</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Preparação de dados &#8211; Parte 1</title>
		<link>./../../../2015/09/02/preparacao-de-dados-parte-1/index.html</link>
		<comments>./../../../2015/09/02/preparacao-de-dados-parte-1/index.html#respond</comments>
		<pubDate>Thu, 03 Sep 2015 02:28:06 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Aprendizado de Máquina]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">http://www.flaviobarros.net/?p=723</guid>
		<description><![CDATA[A linguagem R oferece ferramentas que podem ser usadas para visualização, modelagem e leitura de bancos de dados. Mas uma de suas características mais importantes é que é uma excelente ferramenta para preparação de dados. Naturalmente, como em outras linguagens, existem alguns truques que podem (e devem!) ser utilizados para melhorar a performance das tarefas, e especialmente no caso do R essas escolhas tem um impacto gigantesco na performance do scripts. Assim, neste post vou...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">A linguagem R oferece ferramentas que podem ser usadas para visualização, modelagem e leitura de bancos de dados. Mas uma de suas características mais importantes é que é uma excelente ferramenta para preparação de dados. Naturalmente, como em outras linguagens, existem alguns truques que podem (e devem!) ser utilizados para melhorar a performance das tarefas, e especialmente no caso do R essas escolhas tem um impacto gigantesco na performance do scripts. Assim, neste post vou apresentar alguns dos truques mais importantes que devem ser utilizados no R e também vou apresentar um método muito importante na preparação que é o split-apply-recombine.</p>
<h2 style="text-align: justify;">1. Usando o apply, lappy, tapply</h2>
<p style="text-align: justify;">Algumas vezes os apply&#8217;s podem deixar o código mais rápido ou no mínimo eliminar alguns for&#8217;s deixando o código mais legível. O fato é que, pelo menos no R, é fortemente aconselhável <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://stackoverflow.com/questions/2908822/speed-up-the-loop-operation-in-r">evitar o uso do for</a></span>. Dessa forma, ao invés de usar o for é melhor executar as interações em matrizes, listas e data.frames usando a família apply. Um exemplo:</p>
<pre class="lang:r decode:true ">matriz &lt;- matrix(round(runif(9,1,10),0),nrow=3)
apply(matriz, 1, sum) ## sum by row
apply(matriz, 2, sum) ## sum by column</pre>
<p style="text-align: justify;">Particularmente neste exemplo você pode não ter um ganho de performance, mas o código pelo menos fica um pouco mais legível.</p>
<p style="text-align: justify;">Já que estamos falando de médias, o tapply é especialmente útil na tarefa de obter médias por grupo. A seguir um exemplo onde você consegue tirar uma média para grupos em uma única linha:</p>
<pre tabindex="0">mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3
Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3
Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4
Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4
Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4
Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1
Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1
Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1
Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4
Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1
Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4
Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6
Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8
Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2</pre>
<p>com</p>
<pre class="lang:r decode:true ">tapply(mtcars$hp, mtcars$cyl, mean)</pre>
<p>e você tem a média da potência por capacidade do cilindro. Esta função é extremamente útil em análise descritiva, principalmente quando você tem uma variável numérica que deve ser resumida em função de outra variável categórica. Entretanto algumas vezes temos listas e não vetores tal que é necessário fazer uso do lapply. Vamos gerar dados de exemplo:</p>
<pre class="lang:r decode:true ">lista &lt;- list(a=c('one', 'tow', 'three'), b=c(1,2,3), c=c(12, 'a'))</pre>
<p>Cada elemento na lista é um vetor. Digamos que você queira saber quantos elementos existem em cada vetor, mas sem usar um for! Vejamos:</p>
<pre class="lang:r decode:true ">lapply(lista, length) ## return a list
sapply(lista, length) ## coerce to a vector</pre>
<pre tabindex="0">$a
[1] 3

$b
[1] 3

$c
[1] 2</pre>
<pre tabindex="0">a b c 
3 3 2</pre>
<h2>2. Split, apply and recombine</h2>
<p style="text-align: justify;">Esta é uma técnica que simplesmente DEVE SER CONHECIDA. É o fundamento por detrás de pacotes como o <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://cran.r-project.org/web/packages/plyr/index.html">plyr</a></span> e <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://cran.r-project.org/web/packages/dplyr/index.html">dplyr</a></span> e é a mesma estratégia utilizada no <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://pt.wikipedia.org/wiki/MapReduce">MapReduce</a></span>. Aqui vou usar somente funções do pacote base mas em outra oportunidade planejo apresentar as vantagens de fazer isso com dplyr. Voltando para o conjunto de dados mtcars, suponha que queiramos ajustar modelos de regressão da variável mpg (milha por galão) contra disp, agrupado por gears, e então comparar os coeficientes das regressões. Como fazemos isso?</p>
<p style="text-align: justify;"><a href="http://www.flaviobarros.net/wp-content/uploads/2013/10/mpg1.png"><img class="aligncenter size-full wp-image-323" src="http://www.flaviobarros.net/wp-content/uploads/2013/10/mpg1.png" alt="mpg" width="480" height="480" srcset="./../../../wp-content/uploads/2013/10/mpg1.png 480w, ./../../../wp-content/uploads/2013/10/mpg1-150x150.png 150w, ./../../../wp-content/uploads/2013/10/mpg1-300x300.png 300w, ./../../../wp-content/uploads/2013/10/mpg1-90x90.png 90w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<pre class="lang:r decode:true ">data &lt;- split(mtcars, mtcars$gear) ## split
fits &lt;- lapply(data, function(x) return(lm(x$mpg~x$disp)$coef)) ## apply
do.call(rbind, fits) ## recombine</pre>
<pre tabindex="0">  (Intercept)      x$disp
3    24.51557 -0.02577046
4    39.56753 -0.12221268
5    31.66095 -0.05077512</pre>
<p>Como você pode ver, em três passos está pronto: separa, aplica e agrega. É uma técnica poderosa que pode ser aplicada em diferentes contextos. Veja que o tapply usa esta estratégia internamente para calcular as médias por grupo: split no grupo, aplica a função média, junta as médias para apresentar no final.</p>
<p>Você conseguiria reproduzir o resultado do tapply com o split e o sapply? Poste nos comentários!</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2015/09/02/preparacao-de-dados-parte-1/feed/index.html</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
	</channel>
</rss>
