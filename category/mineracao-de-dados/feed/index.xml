<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Mineração de Dados &#8211; R Mining</title>
	<atom:link href="./index.html" rel="self" type="application/rss+xml" />
	<link>./../../../index.html</link>
	<description>Mineração de Dados, Estatística, Tecnologia</description>
	<lastBuildDate>Tue, 17 Jan 2017 10:04:59 +0000</lastBuildDate>
	<language>pt-BR</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.7.1</generator>
	<item>
		<title>RECONHECIMENTO DE DÍGITOS ESCRITOS A MÃO – PARTE 3</title>
		<link>./../../../2016/03/14/reconhecimento-de-digitos-escritos-mao-parte-3/index.html</link>
		<comments>./../../../2016/03/14/reconhecimento-de-digitos-escritos-mao-parte-3/index.html#comments</comments>
		<pubDate>Tue, 15 Mar 2016 02:54:32 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Aprendizado de Máquina]]></category>
		<category><![CDATA[Data Mining]]></category>
		<category><![CDATA[Educação]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[Preparação de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">./../../../index.html?p=851</guid>
		<description><![CDATA[Na Parte 1 desse post (que já publiquei faz um tempão!) eu fiz uma classificação de imagens de dígitos escritos a mão usando o k-nn (algoritmo dos vizinhos mais próximos) usando as informações das imagens sem nenhum tipo de tratamento, isto é, sem nenhum método de preparação. Como foi mostrado, o k-nn só foi capaz de classificar razoavelmente bem com com k = 1 e conseguiu uma acurácia de apenas 78%, algo muito distante do que...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">Na <a href="./../../../2014/12/22/reconhecimento-de-digitos-escritos-mao-parte-1/index.html">Parte 1</a> desse post (que já publiquei faz um tempão!) eu fiz uma classificação de imagens de dígitos escritos a mão usando o <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nn</a> (algoritmo dos vizinhos mais próximos) usando as informações das imagens sem nenhum tipo de tratamento, isto é, sem nenhum método de preparação. Como foi mostrado, o <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nn</a> só foi capaz de classificar razoavelmente bem com com k = 1 e conseguiu uma acurácia de apenas 78%, algo muito distante do que ainda pode ser conseguido.</p>
<p style="text-align: justify;">Na <a href="./../../../2015/09/14/reconhecimento-de-digitos-escritos-a-mao-parte-2/index.html">Parte 2</a> eu trabalhei com um método de redução de dimensionalidade, o PCA, e também foram explorados diversos outros classificadores, como o Random Forest, o SVM e etc. O resumo dos resultados foi o seguinte:</p>
<ul style="text-align: justify;">
<li style="text-align: justify;">k-nn com k = 1: 84%</li>
<li style="text-align: justify;">Regressão linear: 14%</li>
<li style="text-align: justify;">Regressão Logística Multinomial: 64%</li>
<li style="text-align: justify;">Árvores de decisão: 24%</li>
<li style="text-align: justify;">RandomForest: 72%</li>
</ul>
<p style="text-align: justify;">E a conclusão geral foi que não foi possível bater o k-NN ou ainda mais chegar aos resultados reportados na literatura, superiores a 95% de acurácia na tarefa. Como foi mencionado anteriormente, os possíveis problemas foram:</p>
<ol style="text-align: justify;">
<li>Conjunto pequeno de imagens.</li>
<li>Modelos com parâmetros default.</li>
</ol>
<p style="text-align: justify;">Assim, nessa última parte vou mostrar como é possível treinar melhores modelos com muito mais dados e como é possível melhorar a performance dos algoritmos com melhores hiperparâmetros. Outro ponto importante que eu queria mostrar também é o uso do pacote <a href="http://caret.r-forge.r-project.org/">caret</a> para automatizar diversas tarefas desse processo.</p>
<h2 style="text-align: justify;">1.Dados e visualização</h2>
<p style="text-align: justify;">Como eu comentei antes, vamos nessa parte tentar utilizar um conjunto maior de imagens. Para tanto, ao invés desse pequeno conjunto de teste e treino que foi fornecido na <a href="./../../../2014/12/22/reconhecimento-de-digitos-escritos-mao-parte-1/index.html">Parte 1</a>, vamos utilizar um conjunto muito maior de images de dígitos escritos a mão, o famoso data set <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>. Se você verificar nesse link, você vai encontrar diversas informações com respeito a esse conjunto de dados e também você pode baixa-lo e utilizar nas análises que se seguem, caso queira reproduzir o que você está vendo aqui. ENTRETANTO, para poupar o seu trabalho e o meu, ao invés de pegar os dados diretamente deste site, vamos utilizar esse mesmo conjunto de dados já tratado e preparado pela equipe do <a href="https://www.kaggle.com/c/digit-recognizer">Kaggle</a>. A vantagem é que o arquivos já estão no formato csv e não será mais necessária a etapa de preparação realizada na <a href="./../../../2014/12/22/reconhecimento-de-digitos-escritos-mao-parte-1/index.html">Parte 1</a> e na <a href="./../../../2015/09/14/reconhecimento-de-digitos-escritos-a-mao-parte-2/index.html">Parte 2</a> desta série. Outra vantagem é que após você rodar estes modelos, se você quiser, você pode submeter seus resultados no Leaderboard para experimentar como funciona esse site de competição.</p>
<p style="text-align: justify;">Na <a href="./../../../2015/09/14/reconhecimento-de-digitos-escritos-a-mao-parte-2/index.html">Parte 2</a>, como eu peguei diretamente as imagens e converti em uma matriz, agora você poderia ficar confuso e se perguntar: mas e aí, como são estas imagens? como eu vou ver se já está em csv? Para você não ficar com dúvida, vamos &#8220;imprimir&#8221; as imagens.</p>
<pre class="lang:r decode:true ">############ Explorando as imagens ###############################
## Contando o número de imagens por dígito
barplot(table(treino$label), ylim = c(0,5000))

## Transformando em uma matriz
treino &lt;- as.matrix(treino)

## Imprimindo uma imagem
matriz_imagem &lt;- matrix(treino[1000,-1], ncol = 28)
matriz_imagem &lt;- matriz_imagem[,28:1] ## invertendo a imagem
image(1:28, 1:28, matriz_imagem, col = c('white', 'black'))</pre>
<p><a href="./../../../wp-content/uploads/2016/03/digito4.png" rel="attachment wp-att-867"><img class="aligncenter size-full wp-image-867" src="./../../../wp-content/uploads/2016/03/digito4.png" alt="digito4" width="480" height="480" srcset="./../../../wp-content/uploads/2016/03/digito4.png 480w, ./../../../wp-content/uploads/2016/03/digito4-150x150.png 150w, ./../../../wp-content/uploads/2016/03/digito4-300x300.png 300w, ./../../../wp-content/uploads/2016/03/digito4-65x65.png 65w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<p>Veja que eu peguei a primeira linha do conjunto de treino, transformei em uma matriz e imprimi a matriz como uma imagem. Nesse caso é o dígito 4. Assim, apesar de agora você estar usando um arquivo em csv preparado eles fizeram a mesma coisa que eu fiz anteriormente. Se você quiser entender melhor como cada imagem virou uma linha dessa tabela dá uma olhada na Parte 1 dessa série. Só para mostrar que os dígitos estão ok, eu vou imprimir uma &#8220;imagem média&#8221;, onde em cada imagem eu tenho um valor médio em cada píxel considerando todas as imagens do conjunto de dados.</p>
<pre class="lang:r decode:true ">## Plotando uma imagem média para cada dígito
## Definindo uma escala de cor, indo do branco ao preto
colors &lt;- c('white','black')
cus_col &lt;- colorRampPalette(colors=colors)

## Plot de cada imagem média
## Divindo a tela
png('todos_digitos.png')
par(mfrow=c(4,3),pty='s',mar=c(1,1,1,1),xaxt='n',yaxt='n')

## Criando um array para armazenar as matrizes de cada imagem média
all_img &lt;- array(dim=c(10,28*28))

## Recuperando todas as imagens por dígito e calculando a média
for(di in 0:9) {
  print(di)
  all_img[di+1,] &lt;- apply(treino[treino[,1]==di,-1],2,sum)
  all_img[di+1,] &lt;- all_img[di+1,]/max(all_img[di+1,])*255
  
  z&lt;-array(all_img[di+1,],dim=c(28,28))
  z&lt;-z[,28:1] ##right side up
  image(1:28,1:28,z,main=di,col=cus_col(256))
}</pre>
<p><a href="./../../../wp-content/uploads/2016/03/todos_digitos-1.png" rel="attachment wp-att-869"><img class="aligncenter size-full wp-image-869" src="./../../../wp-content/uploads/2016/03/todos_digitos-1.png" alt="todos_digitos" width="480" height="480" srcset="./../../../wp-content/uploads/2016/03/todos_digitos-1.png 480w, ./../../../wp-content/uploads/2016/03/todos_digitos-1-150x150.png 150w, ./../../../wp-content/uploads/2016/03/todos_digitos-1-300x300.png 300w, ./../../../wp-content/uploads/2016/03/todos_digitos-1-65x65.png 65w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<p style="text-align: justify;">Até há uma certa variação (por isso que há uma sombra) mas no geral os mesmos píxels tem uma intensidade maior considerando cada dígito diferente que foi escrito na imagem. Isso nos leva a crer que os modelos devem conseguir distinguir um dígito do outro.</p>
<h2 style="text-align: justify;">2. Preparação com PCA</h2>
<p style="text-align: justify;">Depois que você baixar os arquivos train.csv e test.csv do Kaggle já podemos efetuar a leitura dos arquivos e a preparação por meio do PCA. O que vamos fazer é aplicar o PCA e retendo somente o número de componentes necessário para alcançar 95% da variância total. Os detalhes sobre isso eu discuti na Parte 2.</p>
<pre class="lang:r decode:true ">## Leitura dos conjuntos de dados de treino e de teste
treino = read.csv('train.csv', header = T)
teste = read.csv('test.csv', header = T)

############ APlicação do PCA ####################################
## Obtendo componentes principais
pc &lt;- prcomp(treino[,-1])
treino_pc &lt;- pc$x

## Obtendo as variâncias acumuladas
vars = pc$sdev^2
props = vars/sum(vars)
varAcum = cumsum(props)
which.min(varAcum &lt; 0.90)

## Aplicando a rotação nos dados de teste
teste_pc &lt;- predict(pc, newdata = teste)

## Salvando treino e teste com PCA
save(treino_pc, file = 'treino_pc.rda')
save(teste_pc, file = 'teste_pc.rda')</pre>
<p style="text-align: justify;">eu costumo salvar os arquivos após cada etapa de preparação de forma a não precisar realizar o processo posteriormente. Outro ponto importante é que salvando os objetos no formato nativo do R, caso você precise recarregar os dados, o processo é muito mais rápido que a leitura em csv.</p>
<h2 style="text-align: justify;">3. Árvore de Decisão</h2>
<p style="text-align: justify;">Como uma primeira tentativa, vamos utilizar o algoritmo para árvores de decisão do pacote <a href="https://cran.r-project.org/web/packages/rpart/index.html">rpart</a>. Vamos utilizar todas as PC&#8217;s e treinar a árvore em 3/4 dos dados. O teste será realizado no 1/4 que foi separado.</p>
<pre class="lang:r decode:true ">#################################################################
## Teste com árvore de decisão
library(rpart)

## Separando o conjunto treino em dois para avaliação
set.seed(1)
inTrain &lt;- createDataPartition(treino$label, p = 3/4, list = F)
train &lt;- treino_pc[inTrain,]
evaluation &lt;- treino_pc[-inTrain,]

## Data frame de treino e teste
treino_arvore = as.data.frame(cbind(train[,1:784]))
treino_arvore$classes = as.factor(treino$label[inTrain])

## Conjunto de teste para avaliação
teste_arvore = as.data.frame(evaluation[,1:784])

## Criando uma árvore
arvore &lt;- rpart(classes ~ ., data = treino_arvore)

## Calculando a matriz de confusão
confusionMatrix(predict(arvore, teste_arvore, type = 'class'), treino$label[-inTrain])
</pre>
<p>e o resultado da matriz de confusão:</p>
<pre class="lang:r decode:true ">Confusion Matrix and Statistics

          Reference
Prediction   0   1   2   3   4   5   6   7   8   9
         0 637   2  13  20   3  66  17  12   0   2
         1   0 995   9   7  28   7   3  75   6  53
         2  60  54 741  61  30  97 130  16  93  10
         3 185  45 111 803  13 270 106  29  89  30
         4   3   0  16  10 796  73  16  82  16 471
         5  86  13  46  88  36 341  31  61 185  49
         6  24  16  47  45  21  40 725   4  13  43
         7  17   0   4   2  20  16   2 652  10  85
         8  18  46  40  27  11  60  12  18 532  13
         9  14   0   6  10  74   6  13 103  57 305

Overall Statistics
                                         
               Accuracy : 0.6217         
                 95% CI : (0.6124, 0.631)
    No Information Rate : 0.1115         
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
                                         
                  Kappa : 0.5796         
 Mcnemar's Test P-Value : &lt; 2.2e-16</pre>
<p style="text-align: justify;">mostra que o desempenho ainda está longe do satisfatório. Com uma acurácia global de apenas 62% estamos ainda muito longe da meta de 95%. Veja que utilizamos a estratégia <a href="http://stats.stackexchange.com/questions/104713/hold-out-validation-vs-k-fold-validation">holdout</a>, e com relação a <a href="./../../../2015/09/14/reconhecimento-de-digitos-escritos-a-mao-parte-2/index.html">Parte 2</a> desta série a única mudança é o fato de estarmos trabalhando com imagens com mais resolução e um conjunto maior. Parece que isso ainda não é o suficiente, assim vamos explorar outros algoritmos e vamos utilizar métodos de validação cruzada para encontrar os melhores hiperparâmetros.</p>
<h2>4. RandomForest</h2>
<p style="text-align: justify;">O <a href="https://en.wikipedia.org/wiki/Random_forest">randomforest</a> é um dos algoritmos de machine learning <a href="https://www.quora.com/What-are-the-top-10-data-mining-or-machine-learning-algorithms">mais utilizados na indústria</a>. Seu sucesso advém do fato de ser robusto, facilmente paralelizável e apresentar um desempenho muito bom em uma grande quantidade de problemas diferentes. Assim, vamos experimentar esse classificador procurando ajustar os melhores hiperperâmetros por validação cruzada. No caso do RF temos que definir qual o melhor m, um parâmetro que determina quantas variáveis são sorteadas na escolha do split em cada nó, de cada árvore de decisão do comitê. Se não ficou claro para você o que significa este hiperparâmetro não tem problema, não é difícil encontrar material onde você pode entender os detalhes do RF. O importante aqui é você entender que o valor do hiperparâmetro será escolhido com base no próprio conjunto de dados, utilizando validação cruzada.</p>
<pre class="lang:r decode:true">#################################################################
## Teste com RandomForest
library(randomForest)

## Separando o conjunto treino em dois para avaliação
set.seed(1)
inTrain &lt;- createDataPartition(treino$label, p = 3/4, list = F)
train &lt;- treino_pc[inTrain,]
evaluation &lt;- treino_pc[-inTrain,]

## Data frame de treino e teste, aqui retendo somente 160 PC's, equivalente a 95% de ## variância.
treino_arvore = as.data.frame(cbind(train[,1:160]))
treino_arvore$classes = as.factor(treino$label[inTrain])

## Conjunto de teste para avaliação
teste_arvore = as.data.frame(evaluation[,1:160])

## Modelagem
fitControl &lt;- trainControl(method = "oob", verboseIter = T,
                           
                           ## Estimate class probabilities
                           classProbs = F)

set.seed(825)
rfFit &lt;- train(classes ~ ., data = treino_arvore, verbose = T,
                method = "rf",
                trControl = fitControl,
                tuneLength = 8,
                metric = "Accuracy")
save(rfFit, file = 'rfFit.rda')
                
## Calculando a matriz de confusão
confusionMatrix(predict(rfFit, teste_arvore, type = 'raw'), treino$label[-inTrain])
</pre>
<p>e a matriz de confusão:</p>
<pre class="lang:r decode:true ">Confusion Matrix and Statistics

          Reference
Prediction    0    1    2    3    4    5    6    7    8    9
         0 1048    0    3    0    1    1    1    0    0    1
         1    0 1130    2    0    1    1    0    2    3    0
         2    0    3 1021   10    4    2    1    6    2    0
         3    0    1    5 1073    3   12    0    1    8   11
         4    1    0    6    0  970    4    1    1    4   18
         5    0    0    0    8    2  932    6    0    4    6
         6    6    1    3    2    5    3 1023    1    2    0
         7    1    1    4    4    0    0    0 1077    3    7
         8    0    0    5    4    5    2    1    2  967    4
         9    2    2    3    2   11    2    0    1    2 1020

Overall Statistics
                                          
               Accuracy : 0.9774          
                 95% CI : (0.9744, 0.9802)
    No Information Rate : 0.1084          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
                                          
                  Kappa : 0.9749</pre>
<p>e enfim chegamos a 97%!</p>
<h2 style="text-align: justify;">5. SVM (Máquina de vertores de suporte)</h2>
<p style="text-align: justify;">O SVM é um algoritmo do tipo &#8220;caixa preta&#8221;. O princípio por detrás do algoritmo é criar hiperplanos separadores em dimensões maiores do que as presentes no conjunto de dados. A ideia é que se os pontos são linearmente separáveis, isto é, se um hiperplano como fronteira de decisão conseguiria separar completamente as classes, então o SVM é um método que pode ser utilizado para encontrar esse hiperplano. ENTRETANTO, ocorre que muitos problemas não são linearmente separáveis, e ainda que fossem não valeria a pena usar o SVM. Quando o problema não é linearmente separável o SVM, de uma certa forma, projeta os dados em um espaço onde é possível criar um hiperplano separador. Também, ele aceita um certo grau de &#8220;impurezas&#8221; dentro das fronteiras de decisão. Enfim, é um algoritmo do tipo &#8220;caixa preta&#8221;, que não tem origem na estatística já que é basicamente um algoritmo de otimização. Mas o fato é que o SVM apresenta resultados muito bons e uma grande quantidade de problemas e vamos ver isso aqui nesse teste.</p>
<pre class="lang:r decode:true">#################################################################
## Teste com SVM RBF
## Carregando os pacotes
library(caret)

## Modelagem
fitControl &lt;- trainControl(method = "cv", verboseIter = T,
                           
                           ## Estimate class probabilities
                           classProbs = F)

set.seed(825)
svmFit &lt;- train(classes ~ ., data = treino_arvore,
                method = "svmRadial",
                trControl = fitControl,
                preProc = c("center", "scale"),
                tuneLength = 8,
                metric = "Accuracy")
save(svmFit, file = 'svmFit2.rda')

## Calculando a matriz de confusão
confusionMatrix(predict(svmFit, teste_arvore, type = 'raw'), treino$label[-inTrain])
</pre>
<p>e avaliando no conjunto de avaliação:</p>
<pre class="lang:r decode:true ">Confusion Matrix and Statistics

          Reference
Prediction    0    1    2    3    4    5    6    7    8    9
         0 1046    0    3    0    1    3    5    1    3    6
         1    1 1128    1    2    2    0    1    3    2    1
         2    0    3 1015   13    4    3    1   15    4    1
         3    1    1    4 1058    0   13    0    1    7    9
         4    2    2   10    1  972    1    4   10    1   21
         5    0    1    2   12    0  924    7    0    3    5
         6    5    0    2    1    5   10 1010    0    4    0
         7    0    0    5    7    0    0    0 1051    0   13
         8    3    2    9    4    0    2    5    1  967    3
         9    0    1    1    5   18    3    0    9    4 1008

Overall Statistics
                                          
               Accuracy : 0.9696          
                 95% CI : (0.9661, 0.9728)
    No Information Rate : 0.1084          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
                                          
                  Kappa : 0.9662</pre>
<p>e novamente conseguimos algo em torno de 97%. Para ser honesto, depois que eu criei os modelos com os melhores hiperparâmetros, submetendo no Kaggle o SVM supera os 98%. Foi o modelo com melhor desempenho nessa tarefa.</p>
<h2>Conclusão</h2>
<p style="text-align: justify;">Acho que depois da Parte 1, Parte 2 e Parte 3 (esta aqui!) você viu como se trabalha com classificação de imagens, como se prepara esse tipo de dado e como é possível alcançar altíssima acurácia utilizando os modelos de machine learning que você encontra por aí. Se você for reproduzir os exemplos, fique ciente que a etapa de modelagem pode demorar muito. No meu caso alguns destes modelos demoraram mais de 8 horas para o ajuste com os melhore hiperparâmetros! Outro ponto que eu não abordei é como o caret seleciona os melhores hiperparâmetros por validação cruzada. Isso vou deixar para falar com maior detalhe em outra oportunidade. Também gostaria de salientar que uma modelagem como essa é algo tipicamente diferente do que se espera em uma análise estatística tradicional. Aqui não estávamos interessados em inferência, mas sim em produzir modelos com o maior poder preditivo possível. Em casos como esse, trabalhar com métodos &#8220;caixa preta&#8221; não é em si um problema. O ponto principal é ter certeza que seu modelo apresentará um bom desempenho no futuro, com novos dados. POR FIM, esses modelos não são o estado da arte nesta tarefa, já que com <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> é possível passar dos 99% de acurácia.</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2016/03/14/reconhecimento-de-digitos-escritos-mao-parte-3/feed/index.html</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Livros recomendados &#8211; Data Science</title>
		<link>./../../../2016/03/12/livros-recomendados-data-science/index.html</link>
		<comments>./../../../2016/03/12/livros-recomendados-data-science/index.html#comments</comments>
		<pubDate>Sat, 12 Mar 2016 17:46:52 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Aprendizado de Máquina]]></category>
		<category><![CDATA[Educação]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">./../../../index.html?p=858</guid>
		<description><![CDATA[Ao longo dos últimos anos, trabalhando com pesquisa na pós-graduação, como estudante de Estatística e como um analista, eu venho consultando e estudando diversos materiais, de artigos em papers até livros sobre Data Mining, Data Science, Estatística, Big Data e etc. Eu tive oportunidade de consultar muitos bons livros, alguns menos e muitos que eram realmente ruins. ASSIM, nesse post eu gostaria de apresentar a minha seleção de livros e uma breve explicação de porque...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">Ao longo dos últimos anos, trabalhando com pesquisa na pós-graduação, como estudante de Estatística e como um analista, eu venho consultando e estudando diversos materiais, de artigos em <em>papers</em> até livros sobre Data Mining, Data Science, Estatística, Big Data e etc. Eu tive oportunidade de consultar muitos bons livros, alguns menos e muitos que eram realmente ruins. ASSIM, nesse post eu gostaria de apresentar a minha seleção de livros e uma breve explicação de porque eu gosto deles e o que você pode encontrar nesses materiais. Estou falando de livros na perspectiva de alguém que trabalha com aplicações, mas não é uma revisão extensiva da literatura da área. Quem visita esse blog já deve ter percebido que eu uso bastante o R e de fato minha lista tem um certo viés indicando livros que usam essa ferramenta. Vamos aos livros!</p>
<h2 style="text-align: justify;">1. Probabilidade e Estatística</h2>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/03/devore.jpg" rel="attachment wp-att-859"><img class="wp-image-859 alignleft" src="./../../../wp-content/uploads/2016/03/devore.jpg" alt="capa Estatística_cengage.cdr" width="137" height="185" srcset="./../../../wp-content/uploads/2016/03/devore.jpg 600w, ./../../../wp-content/uploads/2016/03/devore-222x300.jpg 222w" sizes="(max-width: 137px) 100vw, 137px" /></a> Eu consultava esse livro quando estava estudando Inferência I no bacharelado em Estatística. Não é um livro que geralmente é utilizado em cursos de Estatística (esse não estava na bibliografia) pois apresenta a estatística básica em um nível mais conceitual, praticamente sem demonstrações. JUSTAMENTE POR ISSO eu acho uma excelente indicação para entender estatística básica. TODOS os exemplos e exercícios são com dados reais, de pesquisas reais, nas áreas de engenharia e ciências, o que dá um gostinho a mais já que o leitor consegue ver exatamente como a estatística é aplicada na vida real. O livro cobre probabilidade, testes de hipótese, IC&#8217;s, teste de independência, ANOVA e etc. Está tudo aí, apresentado de forma muito intuitiva sempre com vistas nas aplicações. Um livro realmente muito bom, que para mim na época, fazendo um curso teórico de inferência, trouxe bastante da intuição sobre os métodos que eu estava estudando.</p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/03/uma_senhora_toma_cha.jpg" rel="attachment wp-att-862"><img class=" wp-image-862 alignleft" src="./../../../wp-content/uploads/2016/03/uma_senhora_toma_cha.jpg" alt="capa - uma senhora toma cha_18-02-10.indd" width="140" height="201" srcset="./../../../wp-content/uploads/2016/03/uma_senhora_toma_cha.jpg 1315w, ./../../../wp-content/uploads/2016/03/uma_senhora_toma_cha-209x300.jpg 209w, ./../../../wp-content/uploads/2016/03/uma_senhora_toma_cha-768x1104.jpg 768w, ./../../../wp-content/uploads/2016/03/uma_senhora_toma_cha-712x1024.jpg 712w" sizes="(max-width: 140px) 100vw, 140px" /></a></p>
<p style="text-align: justify;">Quando eu comecei a ler este livro eu mal conseguia parar! É um livro que começa nos primórdios da estatística, explorando uma anedota que dá o nome ao livro. O autor caminha por toda a Estatística, sem entrar nos detalhes, mas mostrando o que é, o contexto histórico dos pesquisadores da época, aplicações e etc. É um livro muito completo que fornece ao leitor um panorama do que é a Estatística. É um livro que tem uma leitura leve e agradável e é super indicado para quem nunca ouviu falar de estatística ou acredita que a área de resume a fazer gráficos e calcular médias! Você pode baixar uma amostra com a introdução nesse <a href="http://www.zahar.com.br/sites/default/files/arquivos//t1184.pdf">link</a> da editora Zahar. Esse é o livro de leitura mais agradável nessa lista. Uma pequena amostra: &#8220;<em>Tentei aqui algo um pouco menos ambicioso: descrever a revolução estatística na ciência do século XX por intermédio de algumas das pessoas (muitas delas ainda vivas) que nela estiveram envolvidas. Tratei muito superficialmente o trabalho que elas criaram, só para provar como suas descobertas individuais se encaixaram no quadro geral.</em>&#8220;</p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/03/effect_sizes.jpg" rel="attachment wp-att-892"><img class="wp-image-892 alignright" src="./../../../wp-content/uploads/2016/03/effect_sizes.jpg" alt="effect_sizes" width="158" height="224" srcset="./../../../wp-content/uploads/2016/03/effect_sizes.jpg 283w, ./../../../wp-content/uploads/2016/03/effect_sizes-212x300.jpg 212w" sizes="(max-width: 158px) 100vw, 158px" /></a>Nesse momento onde se discute bastante a respeito dos problemas com p-valor, p-hacking e afins eu achei esse livro muito interessante pois ele aborda esta e outras questões do ponto de vista das aplicações. O livro é leve, bem escrito e apresenta para o leitor a importância de entender o tamanho de efeito, como fazer uma análise de poder e como fazer meta análise. Não é um livro carregado de fórmulas e também não é um livro que poderia ser usado para um curso de análise de experimentos. Entretanto, para quem gostaria de entender o que é essa discussão toda sobre p-valor acho que esse livro pode ser muito útil.</p>
<h2 style="text-align: justify;">2. Data Science básico</h2>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/03/datascience_for_business.jpg" rel="attachment wp-att-860"><img class="wp-image-860 alignleft" src="./../../../wp-content/uploads/2016/03/datascience_for_business.jpg" alt="datascience_for_business" width="139" height="183" srcset="./../../../wp-content/uploads/2016/03/datascience_for_business.jpg 500w, ./../../../wp-content/uploads/2016/03/datascience_for_business-228x300.jpg 228w" sizes="(max-width: 139px) 100vw, 139px" /></a> Esse é aquele livro que eu sempre indico para alguém que é leigo, ouviu falar de big data, data mining ou coisa que o valha, e gostaria de saber do que se tratam estas coisas. É um livro pensando para esse público, muito bem escrito por profissionais da área, com ótimos exemplos e praticamente sem matemática. É um livro conceitual e portanto, apesar de apresentar diversos exemplos muito interessantes, o livro não tem códigos ou instruções de como implementar as análises em algum software. O objetivo do livro é responder ao leitor: o que é Data Science? O que eu posso fazer com isso? O que a minha empresa pode ganhar com isso?</p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/03/data_smart.jpg" rel="attachment wp-att-861"><img class=" wp-image-861 alignleft" src="./../../../wp-content/uploads/2016/03/data_smart.jpg" alt="data_smart" width="138" height="173" srcset="./../../../wp-content/uploads/2016/03/data_smart.jpg 260w, ./../../../wp-content/uploads/2016/03/data_smart-239x300.jpg 239w" sizes="(max-width: 138px) 100vw, 138px" /></a> Esse livro é muito interessante também, indo na mesma linha do Data Science for Business, mas mostrando outros exemplos interessantes e que se aprofunda um pouco mais nas técnicas e nos estudos de caso. Nesse livro o leitor é levado a analisar dados reais, mas utilizando simples planilhas eletrônicas como o Excel/LibreOffice Calc. Para não falar que são só planilhas, bem no final ele apresenta um exemplo com o R. No entanto, a ideia geral do livro é mostrar o que é uma análise de dados, o que você tem a ganhar com isso e como fazer isso no Excel. É bem legal e para um leitor que não é da área, mas quer dar um passo além, pondo data science em prática, esse é um livro muito bom.</p>
<h2 style="text-align: justify;">3. Linguagem R</h2>
<p style="text-align: justify;">Aqui eu vou mencionar livros fortemente relacionados ao ensino e uso do R. Alguns são introduções conceituais também mas que utilizam muito o R ao longo do livro.</p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/03/R_for_everyone.jpg" rel="attachment wp-att-863"><img class=" wp-image-863 alignleft" src="./../../../wp-content/uploads/2016/03/R_for_everyone.jpg" alt="R_for_everyone" width="160" height="203" srcset="./../../../wp-content/uploads/2016/03/R_for_everyone.jpg 260w, ./../../../wp-content/uploads/2016/03/R_for_everyone-237x300.jpg 237w" sizes="(max-width: 160px) 100vw, 160px" /></a> Pensando em um livro de introdução à linguagem R eu fiquei em dúvida entre este e um outro. Mas minha sugestão vai para este, uma vez que ele pode ser usado mais tarde como referência. É um livro muito bom, bem escrito e com bons exemplos. NO ENTANTO, existem diversos cursos online de introdução a programação em R que provavelmente eu indicaria ao invés de começar direto pelo livro. MAS cada pessoa aprende diferente, e alguém já versado em outras plataformas pode tirar vantagem da velocidade de aprender diretamente de um livro. Esse é um que eu gosto muito.</p>
<p><a href="./../../../wp-content/uploads/2016/03/data_manipulation_with_R.jpg" rel="attachment wp-att-871"><img class=" wp-image-871 alignleft" src="./../../../wp-content/uploads/2016/03/data_manipulation_with_R.jpg" alt="data_manipulation_with_R" width="157" height="238" srcset="./../../../wp-content/uploads/2016/03/data_manipulation_with_R.jpg 330w, ./../../../wp-content/uploads/2016/03/data_manipulation_with_R-198x300.jpg 198w" sizes="(max-width: 157px) 100vw, 157px" /></a></p>
<p style="text-align: justify;">Depois que alguém aprende o básico da linguagem R eu acredito que o grande salto de qualidade é entender exatamente como funcionam as principais estruturas de dados da linguagem, como o matrix, data.frame, list e etc. Além disso também acho muito importante entender como trabalhar com datas, como consultar bancos de dados relacionais, como alterar a estrutura de tabelas e etc. Enfim, uma série de conhecimentos com relação à manipulação de dados. Tudo isto está aqui nesse livro, que eu considero um dos melhores da série <a href="http://www.springer.com/series/6991">User R!</a>. Uma vez que se perde tanto tempo na etapa de preparação de dados, eu acredito que o conteúdo deste livro é essencial.</p>
<p><a href="./../../../wp-content/uploads/2016/03/The-Art-of-R-Programming-Matloff-Norman.jpg" rel="attachment wp-att-872"><img class="wp-image-872 alignright" src="./../../../wp-content/uploads/2016/03/The-Art-of-R-Programming-Matloff-Norman.jpg" alt="The-Art-of-R-Programming-Matloff-Norman" width="158" height="209" srcset="./../../../wp-content/uploads/2016/03/The-Art-of-R-Programming-Matloff-Norman.jpg 303w, ./../../../wp-content/uploads/2016/03/The-Art-of-R-Programming-Matloff-Norman-227x300.jpg 227w" sizes="(max-width: 158px) 100vw, 158px" /></a></p>
<p style="text-align: justify;">Além das técnicas que você vai aprender no livro anterior, o segundo próximo salto de qualidade que um usuário da linguagem R pode conseguir é aprender a como programar com eficiência no R. Quem já tem experiência com programação em outras linguagens costuma ter hábitos que no R podem deixar os scripts muito lentos. Esse livro é muito interessante neste aspecto, mostrando para o usuário porque isso é um problema e como você pode programar melhor. O livro também fala do processo de manipulação de strings e diversos outros tópicos super interessantes. Acho que é uma leitura obrigatória.</p>
<p><a href="./../../../wp-content/uploads/2016/03/ggplot2.jpg" rel="attachment wp-att-878"><img class="wp-image-878 alignleft" src="./../../../wp-content/uploads/2016/03/ggplot2.jpg" alt="ggplot2" width="159" height="239" srcset="./../../../wp-content/uploads/2016/03/ggplot2.jpg 333w, ./../../../wp-content/uploads/2016/03/ggplot2-200x300.jpg 200w" sizes="(max-width: 159px) 100vw, 159px" /></a></p>
<p style="text-align: justify;">Depois de aprender a usar bem a linguagem, o usuário provavelmente já deve estar versado no processo de gerar visualizações básicas com os gráficos do pacote base. ENTRETANTO, a maioria dos gráficos de altíssima qualidade que você vê por aí, gerados com R, são criados com o ggplot2. Eu acho ESSENCIAL aprender a utilizar este sistema gráfico. Assim, minha sugestão é correr os exemplos desse livro pelo menos uma vez, para entender a famosa &#8220;gramática dos gráficos&#8221; que o ggplot2 implementa. Fiquem atentos que a versão disponível desse livro para compra provavelmente é antiga, e uma nova versão atualizada estará sendo lançada em breve, agora em 2016.</p>
<p><a href="./../../../wp-content/uploads/2016/03/isl.jpg" rel="attachment wp-att-873"><img class="wp-image-873 alignright" src="./../../../wp-content/uploads/2016/03/isl.jpg" alt="isl" width="150" height="226" srcset="./../../../wp-content/uploads/2016/03/isl.jpg 1016w, ./../../../wp-content/uploads/2016/03/isl-199x300.jpg 199w, ./../../../wp-content/uploads/2016/03/isl-768x1157.jpg 768w, ./../../../wp-content/uploads/2016/03/isl-680x1024.jpg 680w" sizes="(max-width: 150px) 100vw, 150px" /></a></p>
<p style="text-align: justify;">Se você pretende trabalhar com Data Science e quer realmente entender como funcionam os algoritmos de machine learning minha sugestão é começar com este livro. É um livro de leitura tranquila, feito justamente para profissionais de outras áreas entenderem e aplicarem estes métodos e escrito por dois caras que eu sou fã. Esse livro é ótimo como uma introdução ao machine learning ou statistical learning onde todos os exemplos são implementados na linguagem R. Apesar de não ser um livro sobre a linguagem, é um livro ótimo caso você queira uma introdução ao assunto que utilize o R. Entretanto não é um daqueles livros de aplicações, é um livro teórico sobre o assunto, um livro que poderia ser utilizado em uma disciplina universitária por exemplo. Dois outros pontos fortes do livro são: 1) está disponível de graça <a href="http://www-bcf.usc.edu/~gareth/ISL/">aqui</a>. 2) Em janeiro os autores costumam oferecer um <a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about">MOOC</a> que é praticamente passar por todo este livro.</p>
<p><a href="./../../../wp-content/uploads/2016/03/ESLII.jpg" rel="attachment wp-att-874"><img class="wp-image-874 alignleft" src="./../../../wp-content/uploads/2016/03/ESLII.jpg" alt="SSS Hastie etal.qxd (Page 1)" width="149" height="224" srcset="./../../../wp-content/uploads/2016/03/ESLII.jpg 827w, ./../../../wp-content/uploads/2016/03/ESLII-200x300.jpg 200w, ./../../../wp-content/uploads/2016/03/ESLII-768x1154.jpg 768w, ./../../../wp-content/uploads/2016/03/ESLII-681x1024.jpg 681w" sizes="(max-width: 149px) 100vw, 149px" /></a></p>
<p style="text-align: justify;">Esse é o irmão mais velho do livro anterior. É uma verdadeira obra de referência na área, só que é um livro que apresenta o conteúdo em um nível que pode estar muito acima daquele estudante que está apenas começando na área de análise de dados. Os próprios autores afirmam que escreveram o &#8220;introduction&#8221; para remediar esse problema. Entretanto é um livro fantástico, super completo, cheio de ótimos exemplos tal que se você quiser um livro para entender todos os detalhes de machine learning (e tiver disposição para isso!) esse é o livro que você deve ter. O livro também é disponibilizado gratuitamente nesse <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">link</a>, mas o livro impresso é de uma qualidade impressionante, vale muito a pena.</p>
<p><a href="./../../../wp-content/uploads/2016/03/applied_predictive_modelling.jpg" rel="attachment wp-att-875"><img class=" wp-image-875 alignleft" src="./../../../wp-content/uploads/2016/03/applied_predictive_modelling.jpg" alt="applied_predictive_modelling" width="147" height="222" /></a></p>
<p style="text-align: justify;">Esse é o livro irmão do &#8220;Introduction of Statistical Learning&#8221;, como os próprio autores afirmam. O livro apresenta exemplos reais de análises utilizando as técnicas apresentadas no &#8220;introduction&#8221;. Você vai ver exemplos de classificação, avaliação de modelos, regressão, etc. Os dados são dados reais utilizados em pesquisas dos autores. O livro é muito interessante para ver como se faz data science na realidade. Cheio de exemplos super interessantes em áreas como quimiometria, detecção de fraude, segmentação de clientes e etc. Os autores abordam problemas como: seleção de atributos, problemas com classe desbalanceada, preenchimento de dados faltantes e etc. O livro portanto tem um enfoque nos aspectos práticos da modelagem e deixa a teoria sobre os modelos utilizados para outros livros. Os autores deste livro são os mesmos autores do pacote <a href="http://caret.r-forge.r-project.org/">caret</a> que é tão utilizado pela comunidade R para automatizar tarefas de modelagem. Eles utilizam o pacote extensivamente ao longo do livro.</p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2016/03/handook_data_mining.jpg" rel="attachment wp-att-890"><img class="wp-image-890 alignleft" src="./../../../wp-content/uploads/2016/03/handook_data_mining.jpg" alt="handook_data_mining" width="151" height="185" srcset="./../../../wp-content/uploads/2016/03/handook_data_mining.jpg 300w, ./../../../wp-content/uploads/2016/03/handook_data_mining-244x300.jpg 244w" sizes="(max-width: 151px) 100vw, 151px" /></a> Por fim, eu gostaria de adicionar esse livro a lista porque é um livro super interessante para discutir em linhas gerais o processo de data mining. Ele é um livro mais na linha do livros clássicos de data mining, mas que conta com muitas aplicações. Neste livro são apresentadas análises práticas de problemas de predição de churn, de fraude, segmentação de clientes, previsão de risco e etc. Só pelos exemplos das aplicações já vale a pena. As aplicações não utilizam o R, mas utilizam SAS, Stata, SPSS dentre outros. É uma boa fonte também para quem quer ver como se faz esse tipo de análise em outros softwares.</p>
<h2 style="text-align: justify;">Conclusão</h2>
<p style="text-align: justify;">Está longe de ser uma lista exaustiva, muitos bons títulos que eu não conheço devem ter ficado de fora, mas são todos livros que fizeram muito a diferença para mim. Também me restringi aos livros que eu realmente li e usei, e muitos aí eu uso como referência até hoje. Eu teria outras sugestões para livros sobre experimentos, livros sobre análise de survey, séries temporais e etc. Entretanto eu coloquei mais uma bibliografia básica sobre o que costuma ser abordado em currículos de cursos de Data Science que eu vejo por aí.</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2016/03/12/livros-recomendados-data-science/feed/index.html</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
		<item>
		<title>Regras de associação: vendas cruzadas e recomendação</title>
		<link>./../../../2016/03/01/regras-de-associacao-vendas-cruzadas-e-recomendacao/index.html</link>
		<comments>./../../../2016/03/01/regras-de-associacao-vendas-cruzadas-e-recomendacao/index.html#comments</comments>
		<pubDate>Wed, 02 Mar 2016 00:00:10 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Data Mining]]></category>
		<category><![CDATA[Educação]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">./../../../index.html?p=841</guid>
		<description><![CDATA[Caros leitores, fizemos um novo hangout na semana passada, desta vez sobre regras de associação. Vocês podem conferir aqui o vídeo: Para resumir, no vídeo falamos um pouco sobre o que são as regras de associação, as aplicações em vendas cruzadas e recomendação e foi apresentado também um exemplo prático da famosa &#8220;market basket analysis&#8221; ou análise de cestas de mercado. O material usado na apresentação, com os slides em PDF e os códigos, está...]]></description>
				<content:encoded><![CDATA[<p>Caros leitores, fizemos um novo hangout na semana passada, desta vez sobre regras de associação. Vocês podem conferir aqui o vídeo:</p>
<p><iframe width="640" height="360" src="https://www.youtube.com/embed/yg9DRPWi524?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<p style="text-align: justify;">Para resumir, no vídeo falamos um pouco sobre o que são as regras de associação, as aplicações em vendas cruzadas e recomendação e foi apresentado também um exemplo prático da famosa &#8220;market basket analysis&#8221; ou análise de cestas de mercado.</p>
<p style="text-align: justify;">O material usado na apresentação, com os slides em PDF e os códigos, está disponível no <a href="https://github.com/flaviobarros/lombz_association_rules" target="_blank">github</a>. E também gostaria de deixar como sugestão a leitura dos artigos sobre como implementar regras de associação com <a href="https://pt.wikipedia.org/wiki/MapReduce" target="_blank">mapreduce</a> (em Big Data) e o outro artigo sobre como o Youtube usou regras de associação no seu sistema de recomendação de vídeos. Aqui vão os links:</p>
<ul>
<li style="text-align: justify;"><a href="http://www.inf.unibz.it/~ricci/ISR/papers/p293-davidson.pdf" target="_blank">The YouTube Video Recommendation System</a></li>
<li style="text-align: justify;"><a href="http://worldcomp-proceedings.com/proc/p2012/PDP7948.pdf" target="_blank">Apriori-Map/Reduce Algorithm</a></li>
</ul>
<p>Por favor, não deixe de curtir nossa página no Facebook!</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2016/03/01/regras-de-associacao-vendas-cruzadas-e-recomendacao/feed/index.html</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Cluster &#8211; Segmentação de Clientes</title>
		<link>./../../../2016/02/22/cluster-segmentacao-de-clientes/index.html</link>
		<comments>./../../../2016/02/22/cluster-segmentacao-de-clientes/index.html#respond</comments>
		<pubDate>Mon, 22 Feb 2016 07:46:50 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Data Mining]]></category>
		<category><![CDATA[Educação]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">./../../../index.html?p=830</guid>
		<description><![CDATA[OBS: Caros visitantes, curtam a  página do R Mining no Facebook, aqui ao lado! Agradeço muito. Caros leitores do blog, por conta de diversos fatores eu só estou conseguindo postar agora, pela primeira vez esse ano, em fevereiro. Enfim, demorou, mas eu tenho algo que eu acho que pode ser interessante. Um grupo de amigos, arquitetos de soluções em grandes empresas de São Paulo, está organizando alguns hangouts sobre Big Data. Você pode assistir os...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;"><strong>OBS: Caros visitantes, curtam a  página do R Mining no Facebook, aqui ao lado! Agradeço muito.</strong></p>
<p style="text-align: justify;">Caros leitores do blog, por conta de diversos fatores eu só estou conseguindo postar agora, pela primeira vez esse ano, em fevereiro. Enfim, demorou, mas eu tenho algo que eu acho que pode ser interessante. Um grupo de amigos, arquitetos de soluções em grandes empresas de São Paulo, está organizando alguns hangouts sobre Big Data. Você pode assistir os hangouts que estão disponíveis até o momento nesse <a href="https://www.youtube.com/playlist?list=PLACuKp_68Ygn6UxGOwJeZx7T9nD5NZiFt" target="_blank">playlist</a>, ou mesmo ver um por um aqui:</p>
<ul>
<li>BIG DATA &#8211; Buscando as respostas onde elas estão.</li>
</ul>
<p><iframe width="640" height="360" src="https://www.youtube.com/embed/efYAIH5dprc?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<ul>
<li>Falando sobre Arquitetura para Big Data.</li>
</ul>
<p><iframe width="640" height="360" src="https://www.youtube.com/embed/smloHdrdbdY?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<ul>
<li>Cluster &#8211; Segmentação de Clientes</li>
</ul>
<p><iframe width="640" height="360" src="https://www.youtube.com/embed/vkOObS6N7ZM?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<p style="text-align: justify;">Eu tive a oportunidade de ser  comentarista em dois deles.</p>
<p style="text-align: justify;">E o que é esse post então? Bom, como de repente alguém poderia querer ver os códigos utilizados no terceiro vídeo, e até alguns exemplos suplementares, eu vou apresentar aqui o link dos códigos no <a href="https://github.com/flaviobarros/lombz_cluster">Github</a> e também vou destacar o que eu acho interessante sobre a análise.</p>
<ul style="text-align: justify;">
<li style="text-align: justify;">Essa é uma análise simples e o conjunto de dados é bem pequeno;</li>
<li style="text-align: justify;">A ideia é apresentar o conceito de cluster;</li>
<li style="text-align: justify;">Aplicação com segmentação de clientes;</li>
<li style="text-align: justify;">Descobrir o número de clusters pelo dendograma;</li>
<li style="text-align: justify;">Descobrir o número de clusters pelo método do cotovelo;</li>
<li style="text-align: justify;">Após o agrupamento a análise dos grupos provê insights;</li>
</ul>
<p style="text-align: justify;">Aqui naturalmente eu não abordei diversos tópicos no que diz respeito a aplicação da metodologia de clusters, mas como uma introdução acho que foi interessante. De resto, não vou colocar mais detalhes, como eu faço regularmente nas análises aqui, pois já temos os códigos e também o vídeo sobre a discussão.</p>
<p style="text-align: justify;">
]]></content:encoded>
			<wfw:commentRss>./../../../2016/02/22/cluster-segmentacao-de-clientes/feed/index.html</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Curso &#8220;Pratical Machine Learning&#8221; do Coursera</title>
		<link>./../../../2015/09/29/curso-pratical-machine-learning-do-coursera/index.html</link>
		<comments>./../../../2015/09/29/curso-pratical-machine-learning-do-coursera/index.html#comments</comments>
		<pubDate>Wed, 30 Sep 2015 00:53:50 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Aprendizado de Máquina]]></category>
		<category><![CDATA[Data Mining]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[MOOCS]]></category>
		<category><![CDATA[Coursera]]></category>
		<category><![CDATA[Data Science]]></category>

		<guid isPermaLink="false">http://www.flaviobarros.net/?p=760</guid>
		<description><![CDATA[Em mais uma rodada das resenhas de cursos sobre Data Science, desta vez vou apresentar minha avaliação do curso &#8220;Pratical Machine Learning&#8220;, ou Machine Learning Prático. 1. Sobre o que é o curso? Este é o penúltimo curso da especialização em Data Science do Coursera do Coursera. Já falei sobre o último da sequência aqui nesse blog, o Developing Data Products, mas este é o último onde são apresentados conceitos novos de Data Science. Como o...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">Em mais uma rodada das resenhas de cursos sobre Data Science, desta vez vou apresentar minha avaliação do curso &#8220;<span style="color: #0000ff;"><a style="color: #0000ff;" href="https://www.coursera.org/course/predmachlearn">Pratical Machine Learning</a></span>&#8220;, ou Machine Learning Prático.</p>
<h2 style="text-align: justify;">1. Sobre o que é o curso?</h2>
<p style="text-align: justify;">Este é o penúltimo curso da especialização em <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://www.coursera.org/specializations/jhudatascience?utm_medium=courseDescripTop">Data Science do Coursera</a> </span>do Coursera. Já falei sobre o último da sequência aqui nesse blog, o <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://www.flaviobarros.net/2015/08/24/curso-developing-data-products-do-coursera/">Developing Data Products</a></span>, mas este é o último onde são apresentados conceitos novos de Data Science.</p>
<p style="text-align: justify;">Como o nome do curso já diz é um curso sobre <a href="https://pt.wikipedia.org/wiki/Aprendizado_de_m%C3%A1quina">machine learning</a>. Mas e o prático no nome? O que quer dizer? Bem, esse prático na verdade quer dizer que esse curso não tem o aprofundamento teórico que é visto nos de <a href="https://www.coursera.org/course/statinference">Inferência</a> e <a href="https://www.coursera.org/course/regmods">Regressão</a>. Isto é, o objetivo desse curso não é apresentar o campo de machine learning de uma forma completa, mas habilitar o aluno a utilizar estas ferramentas pela primeira vez.</p>
<p style="text-align: justify;">Na verdade, na minha opinião, os objetivos gerais desse curso são:</p>
<ul>
<li style="text-align: justify;">Apresentar a ideia de modelos para predição;</li>
<li style="text-align: justify;">Apresentar a ideia de avaliação de modelos;</li>
<li style="text-align: justify;">Mostrar como usar o <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://caret.r-forge.r-project.org/">caret</a></span>;</li>
</ul>
<p style="text-align: justify;">Veja que durante os cursos de inferência e regressão o foco foi no processo de inferência, ao passo que aqui o foco é no processo de criação de modelos de predição para problemas práticos. Quem já trabalhou com predição sabe que o processo completo começa na preparação, análise exploratória, criação de modelos, avaliação de modelos e implantação. Assim eu posso dizer que neste curso o foco foi em como criar os modelos e avalia-los corretamente.</p>
<p style="text-align: justify;">Com relação a teoria de machine learning, nos vídeos são apresentados os princípios fundamentais dos algoritmos, mas de uma forma bastante superficial, somente o necessário para o aluno ter alguma ideia de como os algoritmos funcionam. Isso não é necessariamente uma deficiência do curso, mas sim uma opção dos instrutores e algo que é colocado antecipadamente.</p>
<p style="text-align: justify;">Quanto o pacote <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://caret.r-forge.r-project.org/">caret</a></span> ele é usado extensivamente aqui. É um pacote que eu uso muito, há muito tempo, e realmente é essencial para automatizar aquelas tarefas repetitivas presentes na pipeline de criação de modelos de predição.</p>
<h2 style="text-align: justify;">2. Pré-requisitos</h2>
<p style="text-align: justify;">Nessa altura da especialização é um curso que demanda bastante o domínio da linguagem R. Até chegar aqui, em tese, o aluno já utilizou bastante a linguagem para preparação, análise exploratória, geração de relatórios e etc. Assim, se você não tem um bom domínio do R vai ter problemas para realizar as tarefas e o projetos. Outro ponto fundamental é que é necessário um conhecimento básico de modelagem e inferência e como utilizar o github. Enfim:</p>
<ul>
<li style="text-align: justify;">Linguagem R;</li>
<li style="text-align: justify;">Git &amp; Github</li>
<li style="text-align: justify;">Regressão e modelagem;</li>
</ul>
<h2>3. Conteúdo</h2>
<p>No geral o conteúdo aborda os seguintes tópicos:</p>
<ul>
<li>Criação de modelos preditivos;</li>
<li>Avaliação de modelos;</li>
<li>Automatização com o caret;</li>
<li>Overview dos principais algoritmos de ML;</li>
</ul>
<h2>Minha experiência</h2>
<p style="text-align: justify;">Eu vi todo o material, fiz todos os quizes e o projeto final. Minha impressão é que os vídeos podem não fornecer o suficiente para um aluno que está vendo o assunto pela primeira vez. Digo isto pois eu tive a impressão que as aulas foram algo superficiais, mas seria necessário saber mais para responder corretamente todos os quizes.</p>
<p style="text-align: justify;">Um ponto que eu gostaria de destacar é que o instrutor fala como uma batedeira. Vejam, eu entendo que ele é americano e está falando para uma audiência de língua inglesa, MAS o fato é que uma boa parcela do público é de alunos estrangeiros que não tem o inglês como o idioma nativo. Eu tenho proficiência em inglês para conversar, até consegui fazer o curso sem utilizar as legendas, mas acredito que o autor poderia fazer um esforço para facilitar a vida dos estrangeiros, minha opinião ;-). Só para comparar, no curso de Machine Learning do Andrew Ng por exemplo, você vê que há um esforço do instrutor em se fazer entender para uma ampla audiência.</p>
<p style="text-align: justify;">O curso tem bastante material e os exercícios demandam trabalho. Não é algo que faz em dois dias e pronto. Tem que dar uma raladinha, principalmente se você não tem experiência com o R ;-). Eu diria que um aluno regular pode ter que gastar algo entre 4h a 10h por semana.</p>
<h2 style="text-align: justify;">Conclusão</h2>
<p style="text-align: justify;">Bom, para quem estiver fazendo a especialização não tem jeito, tem que fazer mesmo. Mas e para quem não está? Vale a pena fazer só esse? Eu particularmente acho que vale em alguns casos. O curso tem suas deficiências, questões que podem ser melhoradas, mas se eu tivesse que destacar um grande mérito seria o seguinte: você vai aprender a aplicar data science usando o R como plataforma.</p>
<p style="text-align: justify;">Acredito que pode ser um curso bastante útil para profissionais de data science que querem migrar de plataforma; estudantes de graduação que desejam ver aplicações e entusiastas que pretendem participar de competições e modelagem. É  um bom começo.</p>
<p style="text-align: justify;">Devo salientar que o curso está pra lá de longe de oferecer um conteúdo aprofundado sobre os aspectos de ML. Entretanto essa não é a proposta do curso e já existem dois bons complementos para esse curso, para quem pretende aprender mais sobre ML:</p>
<ol>
<li style="text-align: justify;"><span style="color: #0000ff;"><a style="color: #0000ff;" href="https://www.coursera.org/learn/machine-learning">Machine Learning</a></span> do Andrew Ng;</li>
<li style="text-align: justify;"><span style="color: #0000ff;"><a style="color: #0000ff;" href="https://lagunita.stanford.edu/courses/HumanitiesandScience/StatLearning/Winter2015/about">Statistical Learning</a></span> do Trevor Hastie and Robert Tibshirani;</li>
</ol>
<p style="text-align: justify;">O primeiro foi o curso que inicou o Coursera, do próprio fundador, e o segundo é um curso fantástico, que deve ser oferecido agora novamente no verão, e que tem todo seu conteúdo disponível no <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/">Youtube</a></span> para quem se interessar.</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2015/09/29/curso-pratical-machine-learning-do-coursera/feed/index.html</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Reconhecimento de dígitos escritos a mão – PARTE 2</title>
		<link>./../../../2015/09/14/reconhecimento-de-digitos-escritos-a-mao-parte-2/index.html</link>
		<comments>./../../../2015/09/14/reconhecimento-de-digitos-escritos-a-mao-parte-2/index.html#comments</comments>
		<pubDate>Mon, 14 Sep 2015 12:00:02 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Aprendizado de Máquina]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[Preparação de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">http://www.flaviobarros.net/?p=746</guid>
		<description><![CDATA[Na Parte 1 desse post (que já publiquei faz um tempão!) eu fiz uma classificação de imagens de dígitos escritos mão usando o k-nn (algoritmo dos vizinhos mais próximos) usando as informações das imagens sem nenhum tipo de tratamento, isto é, sem nenhum método de preparação. Como foi mostrado, o k-nn só foi capaz de classificar razoavelmente bem com com k = 1 e conseguiu uma acurácia de apenas 78%, algo muito distante do que ainda...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">Na <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://www.flaviobarros.net/2014/12/22/reconhecimento-de-digitos-escritos-mao-parte-1/">Parte 1</a></span> desse post (que já publiquei faz um tempão!) eu fiz uma classificação de imagens de dígitos escritos mão usando o <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nn</a> (algoritmo dos vizinhos mais próximos) usando as informações das imagens sem nenhum tipo de tratamento, isto é, sem nenhum método de preparação. Como foi mostrado, o <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nn</a> só foi capaz de classificar razoavelmente bem com com k = 1 e conseguiu uma acurácia de apenas 78%, algo muito distante do que ainda pode ser conseguido.</p>
<p style="text-align: justify;">Assim, nesta segunda parte, vou explorar outras alternativas além do <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nn</a>, e mais do que isso, tentar aplicar algum método de preparação para melhorar a performance dos algoritmos. Aqui vou repetir o procedimento de leitura do outro post, para facilitar a reprodução dessa análise.</p>
<p style="text-align: justify;">OBS: Leia a <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://www.flaviobarros.net/2014/12/22/reconhecimento-de-digitos-escritos-mao-parte-1/">Parte 1</a></span> para entender melhor o problema!</p>
<h3 style="text-align: justify;">1. LEITURA</h3>
<p style="text-align: justify;">Os dados do problema são imagens do tipo <a href="http://en.wikipedia.org/wiki/Netpbm_format"><span style="color: #0000ff;">PGM</span></a>, com 64 x 64 pixels por imagem, onde cada pixel tem valor 1 ou 0, indicando se o pixel é preto ou branco. Cada imagem tem um nome no formato X_ yyy.BMP.inv.pgm, onde o X representa o dígito desenhado na imagem. Os dados estão divididos em um conjunto de treino e um conjunto de teste e podem ser baixados nos seguintes links: <a href="http://www.flaviobarros.net/wp-content/uploads/2014/12/teste.zip">teste</a>  e <a href="http://www.flaviobarros.net/wp-content/uploads/2014/12/treino.zip">treino</a></p>
<p style="text-align: justify;">Assim a primeira parte do problema é efetuar a leitura dos dados. Para isso me utilizarei do pacote <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://cran.r-project.org/web/packages/pixmap/index.html">pixmap</a></span> com o qual é possível ler e manipular imagens PGM. A seguir, o processo de leitura das imagens e a criação de um vetor com as respostas, isto é, com o número do dígito que está escrito na imagem.</p>
<pre class="lang:r decode:true">## Carregando o pacote pixmap
library(pixmap)

## Definindo o diretório com as imagens de treino
path_treino &lt;- '/sua/pasta/treino/'

## Define como diretório de trabalho
setwd(path_treino)

## Lê os nomes dos arquivos
files &lt;- dir()

## Retira as classes dos nomes dos arquivos
classes &lt;- as.factor(substring(files,first=1,last=1))

## Cria data.frame para armazenar dados de treino
treino &lt;- as.data.frame(matrix(rep(0,length(files)*64*64), nrow=length(files)))

## Efetua leitura dos dados de treino
for (i in 1:length(files)) {

  ## Lendo as imagens
  x &lt;- read.pnm(files[i])

  ## No slot 'grey' está a matriz de pixels que é retirada e vetorizada
  treino[i,] &lt;- as.vector(x@grey, mode='integer')
}

## Define como diretório de trabalho o local das imagens para teste
path_teste &lt;- '/sua/pasta/teste/'

## Diretório de trabalho
setwd(path_teste)

## Lê os nomes dos arquivos
files &lt;- dir()

## Classes
predic &lt;- as.factor(substring(files,first=1,last=1))

## Cria data.frame para armazenar conjunto de teste
teste &lt;- as.data.frame(matrix(rep(0,length(files)*64*64), nrow=length(files)))

## Leitura do conjunto de teste
for (i in 1:length(files)) {
  x &lt;- read.pnm(files[i])
  teste[i,] &lt;- as.vector(x@grey, mode='integer')
}</pre>
<p style="text-align: justify;">É importante observar que a matriz de pixels fica armazenada no slot @grey, e que após a leitura é transformada em um vetor, tal que o data.frame final fica com 64&#215;64 colunas e 1949 linhas (o total de imagens). O conjunto de  teste tem somente 50 imagens, logo o data.frame vai ficar com 64&#215;64 colunas e somente 50 linhas. Em suma, cada coluna é um píxel e cada linha uma das diferentes imagens.</p>
<h2 style="text-align: justify;">Aplicação do PCA</h2>
<p style="text-align: justify;">O primeiro procedimento que vou utilizar aqui para tentar melhorar a performance da classificação, é usar o método de <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://pt.wikipedia.org/wiki/An%C3%A1lise_de_Componentes_Principais">Análise de Componente Principais</a></span>, para fazer a rotação do sistema de coordenadas e então ficar somente com uma parte das componentes principais. No fim das contas, o que estaremos fazendo é uma redução do número de atributos pois preservando somente parte das componentes principais eu estarei reduzindo o número de colunas da matriz de dados.</p>
<p style="text-align: justify;">A ideia por trás desse procedimento é que nem todos os atributos (que nesse  caso são pixels) tem a mesma importância para entender o que está escrito na imagem. Pense que pixels que estão próximos da borda podem ter uma importância menor ao passo que pixels mais ao centro devem ser bem mais importantes. Veja que se tivéssemos 10 atributos (ou variáveis), digamos X1 X2 X3 &#8230; X10, então pela rotação do sistema obteríamos PC1 PC2 PC3 &#8230; PC10. A diferença é que as componentes principais, que serão utilizadas no lugar dos atributos originais, são ordenadas de acordo com a variância capturada, isto  é, a PC1 captura maior variância que a PC2 e assim por diante. Dessa forma poderemos escolher as primeiras que capturam a maior variância desejada, reduzindo assim o número de atributos que entram no modelo.</p>
<p style="text-align: justify;">Um detalhe importante da aplicação desse método é que após encontrar as componentes principais no conjunto de treino DEVEMOS APLICAR A MESMA ROTAÇÃO no conjunto de teste. Essa matriz de rotação DEVE SER A MESMA OBTIDA NO CONJUNTO DE TREINO, uma vez que eu não quero usar um método de redução de atributos que tenha utilizado qualquer informação do conjunto de testes. Isso é importante para garantir que a acurácia global inferida será o mais próximo possível do acurácia do modelo em novas imagens.</p>
<h3 style="text-align: justify;">PCA com prcomp</h3>
<p style="text-align: justify;">Para aplicar o PCA agora vou utilizar a função do pacote base <a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html">prcomp</a>. Com ela vamos obter as componentes principais, no caso o mesmo número de colunas do conjunto de dados original, e também o desvio padrão de cada uma delas. Com essas duas informações poderemos avaliar a variância acumulada das componentes principais e escolher somente as primeiras que contemplem, vamos dizer, 90% da variância nesse conjunto de dados.</p>
<pre class="lang:r decode:true">############ APlicação do PCA ####################################
## Obtendo componentes principais
pc &lt;- prcomp(treino)
treino_x &lt;- pc$x

## Obtendo as variâncias acumuladas
vars = pc$sdev^2

## Calculando a proporção de variância 
props = vars/sum(vars)

## Obtendo as variâncias acumulada
varAcum = cumsum(props)

## Obtendo o número de componentes que capturam pelo menos 90% da variância.
which.min(varAcum &lt; 0.9)

## Aplicando a rotação nos dados de teste
test.p &lt;- predict(pc, newdata = teste)</pre>
<p style="text-align: justify;">Com 341 componentes principais capturamos 90% da variância. Isto é uma redução agressiva no número de atributos, uma vez que tínhamos mais de 1900 pixels. Agora estamos prontos para aplicar o método k-nn novamente, mas desta vez usando as 400 componentes principais como input (valor aproximado). Esperamos uma melhora.</p>
<h2 style="text-align: justify;">K-nn com PCA</h2>
<pre class="lang:r decode:true">## Treinando o knn com as 341 variáveis
predito &lt;- knn(train=pc$x[,1:400], test=test.p[,1:400], cl=classes, k=1, prob=T)

result &lt;- data.frame(cbind(predic, predito, acerto = predic==predito))

sum(result$acerto)/nrow(result)</pre>
<p style="text-align: justify;">Com k = 1 obtivemos uma performance de 84% de acurácia que já é uma melhora. Há que se notar também que não adianta muito colocar mais componentes e que também outros valores de k parecem não trazer grandes melhoras. Talvez seja a hora de partir para outros tipos de modelos a partir daqui. A seguir o gráfico do desempenho do k-nn ao longo dos valores de k com a inclusão de novas componentes.</p>
<p style="text-align: justify;"><a href="http://www.flaviobarros.net/wp-content/uploads/2015/09/valores_dim.png"><img class="aligncenter size-full wp-image-749" src="http://www.flaviobarros.net/wp-content/uploads/2015/09/valores_dim.png" alt="valores_dim" width="480" height="480" srcset="./../../../wp-content/uploads/2015/09/valores_dim.png 480w, ./../../../wp-content/uploads/2015/09/valores_dim-150x150.png 150w, ./../../../wp-content/uploads/2015/09/valores_dim-300x300.png 300w, ./../../../wp-content/uploads/2015/09/valores_dim-65x65.png 65w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<h2 style="text-align: justify;">Árvore de decisão com PCA</h2>
<p style="text-align: justify;">Parece que não vamos muito mais longe com o k-nn, assim vamos tentar outros métodos como as árvores de decisão. Aqui vou usar a implementação do algoritmo <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">CART</a> no pacote <a href="https://cran.r-project.org/web/packages/rpart/index.html">rpart</a> do R. Vou usar como entrada as 400 componentes principais.</p>
<pre class="lang:r decode:true ">## Data frame de treino e teste
treino_arvore = as.data.frame(cbind(treino_pc[,1:544]))
treino_arvore$classes = classes

teste_arvore = as.data.frame(teste_pc[,1:544])

## Teste com árvore de decisão
library(rpart)

## Criando uma árvore
arvore &lt;- rpart(classes ~ ., data = treino_arvore)

## Fazendo a previsão no conjunto de teste
predito &lt;- predict(object = arvore, newdata = teste_arvore, type = 'class')

## Resultado
result &lt;- data.frame(cbind(predic, predito, acerto = predic==predito))

## Cálculo da taxa de acerto
sum(result$acerto)/nrow(result)</pre>
<p>O resultado foi inferior ao k-nn, com somente 72% de acurácia global.</p>
<p>A seguir vou apresentar ainda mais algumas tentativas com uma regressão linear, regressão logística multinomial e o randomForest.</p>
<h2>Outros modelos com PCA</h2>
<pre class="lang:r decode:true ">## Teste com logística multinomial
library(nnet)

## Ajustando um modelo
multinomial &lt;- multinom(classes ~ ., data = treino_arvore, MaxNWts = 5460)

## Fazendo a previsão no conjunto de teste
predito &lt;- predict(object = multinomial, newdata = teste_arvore, type = 'class')

## Resultado
result &lt;- data.frame(cbind(predic, predito, acerto = predic==predito))

## Cálculo da taxa de acerto
sum(result$acerto)/nrow(result)

#################################################################
## Teste com randomForest
library(randomForest)

## Ajustando um modelo
rf &lt;- randomForest(classes ~ ., data = treino_arvore)

## Fazendo a previsão no conjunto de teste
predito &lt;- predict(object = rf, newdata = teste_arvore, type = 'class')

## Resultado
result &lt;- data.frame(cbind(predic, predito, acerto = predic==predito))

## Cálculo da taxa de acerto
sum(result$acerto)/nrow(result)

#################################################################
## Teste com regressão linear
## Data frame de treino e teste
treino_arvore = as.data.frame(cbind(treino_pc[,1:544]))
treino_arvore$classes = classes

teste_arvore = as.data.frame(teste_pc[,1:544])

## Criando uma árvore
reg &lt;- lm(classes ~ ., data = treino_arvore)

## Fazendo a previsão no conjunto de teste
predito &lt;- predict(object = fit, newdata = teste_arvore)
predito &lt;- round(predito, 0)

## Resultado
result &lt;- data.frame(cbind(predic, predito, acerto = predic==predito))

## Cálculo da taxa de acerto
sum(result$acerto)/nrow(result)</pre>
<p>Com desempenhos inferiores ou iguais ao k-nn.</p>
<h2>Resumos dos resultados</h2>
<ul>
<li>k-nn com k = 1: 84%</li>
<li>Regressão Linear: 14%</li>
<li>Regressão Logística Multinomial: 64%</li>
<li>Árvore de Decisão: 34%</li>
<li>RandomForest: 72%</li>
</ul>
<h2>Conclusão</h2>
<p style="text-align: justify;">Por enquanto parece que não conseguimos passar do teto de 84% com o k-nn. Isso pode acontecer por diversos motivos, mas alguns possíveis responsáveis por ainda não alcançarmos os melhores resultados são:</p>
<ol style="text-align: justify;">
<li>No conjunto original teríamos mais de 60 mil imagens para treinar os algoritmos;</li>
<li>O conjunto de teste é muito pequeno logo há muita variabilidade na estimativa de erro;</li>
<li>Usamos parâmetros default em todos os modelos;</li>
</ol>
<p style="text-align: justify;">No próximo post vamos tentar encontrar os melhores hiperparâmetros para alguns modelos, tentar avaliar melhor o desempenho dos algoritmos, tentar novos algoritmos e automatizar tudo com o <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://caret.r-forge.r-project.org/">caret</a></span>.</p>
<p style="text-align: justify;">Gostou do artigo? Curta nossa página no Facebook!</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2015/09/14/reconhecimento-de-digitos-escritos-a-mao-parte-2/feed/index.html</wfw:commentRss>
		<slash:comments>7</slash:comments>
		</item>
		<item>
		<title>Preparação de dados &#8211; Parte 2</title>
		<link>./../../../2015/09/07/preparacao-de-dados-parte-2/index.html</link>
		<comments>./../../../2015/09/07/preparacao-de-dados-parte-2/index.html#respond</comments>
		<pubDate>Mon, 07 Sep 2015 13:00:37 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[Preparação de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">http://www.flaviobarros.net/?p=730</guid>
		<description><![CDATA[Neste post eu vou falar sobre como trabalhar com GRANDES ARQUIVOS DE TEXTO em chunks no R. Esse pode ser um problema complicado e que pode aparecer na vida do analista trabalhando com arquivos de log por exemplo. Antes de continuar o post gostaria de salientar que estou utilizando o termo chunk para designar um pedaço do arquivo de texto, isto é, estou dizendo que vamos trabalhar com grandes arquivos de texto, pedaço por pedaço. Mas por que...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">Neste post eu vou falar sobre como trabalhar com GRANDES ARQUIVOS DE TEXTO em <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://www.wordreference.com/enpt/chunk">chunks</a></span> no R. Esse pode ser um problema complicado e que pode aparecer na vida do analista trabalhando com <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://en.wikipedia.org/wiki/Logfile">arquivos de log</a></span> por exemplo. Antes de continuar o post gostaria de salientar que estou utilizando o termo <em>chunk </em>para designar um pedaço do arquivo de texto, isto é, estou dizendo que vamos trabalhar com grandes arquivos de texto, pedaço por pedaço.</p>
<h2>Mas por que chunks?</h2>
<p style="text-align: justify;">Bom, se você já teve oportunidade de trabalhar com grandes arquivos de dados no R você já deve ter percebido que dependendo do tamanho do arquivo o simples procedimento de leitura pode demorar muito tempo. Isso ocorre porque o R trabalha com arquivos de dados diretamente na memória RAM tal que arquivos gigantescos ou podem ocupar toda a memória RAM do seu computador, inviabilizando a leitura, ou mesmo podem até ser lidos mas deixarem o computador extremamente lento tomando muito tempo da análise.</p>
<p style="text-align: justify;">Para mostrar um exemplo prático de como trabalhar com este tipo de arquivo no R vamos utilizar um conjunto de dados muito interessante com informações de vôos, o <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://stat-computing.org/dataexpo/2009/the-data.html">Airlines</a></span>. Vamos trabalhar somente com as informações relativas a 1988, mas que sozinhas já são suficiente para dar uma tremenda dor de cabeça. Para trabalhar com esses dados eu vou usar o pacote <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://cran.r-project.org/web/packages/iterators/index.html">iterators</a></span>, um pacote que permite ler um arquivo linha por linha, ou chunk por chunk, mas sem ter que carregar o arquivo inteiro na memória. Para você ter uma ideia de como isso funciona tente rodar esse código:</p>
<pre class="lang:r decode:true">## Instalando e carregando o pacote
install.packages('iterators')
library(iterators)

## Efetuando a conexão de leitura do arquivo, direto do formato bz2 (uma espécie de zip).
con &lt;- bzfile('1988.csv.bz2', 'r')</pre>
<p style="text-align: justify;">Preste atenção que o objeto <strong>con</strong> armazena somente uma CONEXÃO. EU não estou lendo o arquivo ainda e portanto não o estou carregando na memória. Esse tipo de conexão é parecida com a que você poderia criar em um <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://pt.wikipedia.org/wiki/Sistema_de_gerenciamento_de_banco_de_dados">sistema de gerenciamento de banco de dados</a></span> antes de fazer a primeira consulta em SQL.</p>
<p style="text-align: justify;">OK, agora que temos a conexão vamos criar um iterator (ou iterador):</p>
<pre class="lang:r decode:true">## Cria um iterador que lê uma linha do arquivo por vez (n = 1)
it &lt;- ireadLines(con, n=1)

## Comando para ir retornando linha a linha
nextElem(it)
nextElem(it)</pre>
<p style="text-align: justify;">Como você pode ver você está imprimindo no terminal linha por linha do arquivo. Assim, se você quiser trabalhar linha por linha, ou mesmo chunk por chunk (n &gt; 1) você pode ir carregando pequenos pedaços do arquivo que não vão ocupar a memória inteira.</p>
<h2 style="text-align: justify;">Mas como saber qual é a última linha?</h2>
<p style="text-align: justify;">Como estamos lendo linha a linha não temos como saber de antemão quantas linhas devemos ler. Assim a solução para o problema é tentar pegar o erro que ocorre quando chegamos na última linha do arquivo e pedimos mais uma com o nextElem(it). Quando fazemos isso a função retorna um erro, o qual podemos &#8220;transformar&#8221; em um FALSE do R e usar esse sinal em um programa para encerrar o trabalho.</p>
<pre class="lang:r decode:true">tryCatch(expr=nextElem(it), error=function(e) return(FALSE))</pre>
<p>esse comando retorna um  FALSE quando chega no fim do arquivo. Esse é um truque muito útil na preparação com grandes arquivos de texto.</p>
<h2>Exemplo do Stackoverflow</h2>
<p style="text-align: justify;">Há um tempo atrás surgiu uma pergunta no stackoverflow justamente sobre grandes arquivos de texto. Você encontrar a pergunta aqui: <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://pt.stackoverflow.com/questions/35469/pre-processar-grande-arquivo-de-texto-txt-substituir-por/35645#35645" target="_blank">Pre-processar grande arquivo de texto</a></span>. Veja que há também uma solução interessante do Carlos Cinelli (do <a href="http://analisereal.com/" target="_blank">analisereal.com</a>) que não faz uso do iterators. É uma solução que usa o seguinte método: lê 4000 linhas por vez e pula as linhas já lidas anteriormente. Eu acredito que a estratégia com iterators pode ser melhor pois você só vai varrer o arquivo uma vez, isto é, na solução com iterator você tem um apontador para a linha que parou e pode continuar a partir dali, até chegar no final do arquivo. Na solução usando o read.csv() ou read.table() em cada iteração o arquivo é lido novamente e todas as linhas já lidas são puladas.</p>
<pre class="lang:r decode:true ">## Carregando o pacote
library(iterators)

## Defindo a função que troca vírgula por ponto
change_dot &lt;- function(file, saida='teste.txt', chunk=1) {

  ## Cria conexão com arquivo a ser lido
  con1 &lt;- file(file, 'r')

  ## Cria conexão onde será escrito o novo arquivo
  con2 &lt;- file(saida, open = 'w')
  linha &lt;- 0

  ## Criando o iterador
  it &lt;- ireadLines(con1, n=chunk)

  ## Lendo a primeira linha e escrevendo no outro arquivo
  out &lt;- tryCatch(expr=write(x = gsub(pattern = ',', replacement = '.', x = nextElem(it)), con2), 
                   error=function(e) e)

  ## Observe o uso do tryCatch() para saber o fim do arquivo
  while(!any(class(out) == "error")) {
    linha = linha + 1
    print(paste('Escrita linha ', linha))
    out &lt;- tryCatch(expr=write(x = gsub(pattern = ',', replacement = '.', x = nextElem(it)), con2, append = T), 
                  error=function(e) e)
  }
}</pre>
<p>Nesse caso em específico o tempo total para a realização da tarefa foi de aproximadamente 6 segundos.</p>
<p>Gostou do artigo ou achou a dica útil?! Curta nossa página no Facebook!</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2015/09/07/preparacao-de-dados-parte-2/feed/index.html</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Preparação de dados &#8211; Parte 1</title>
		<link>./../../../2015/09/02/preparacao-de-dados-parte-1/index.html</link>
		<comments>./../../../2015/09/02/preparacao-de-dados-parte-1/index.html#respond</comments>
		<pubDate>Thu, 03 Sep 2015 02:28:06 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Aprendizado de Máquina]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>

		<guid isPermaLink="false">http://www.flaviobarros.net/?p=723</guid>
		<description><![CDATA[A linguagem R oferece ferramentas que podem ser usadas para visualização, modelagem e leitura de bancos de dados. Mas uma de suas características mais importantes é que é uma excelente ferramenta para preparação de dados. Naturalmente, como em outras linguagens, existem alguns truques que podem (e devem!) ser utilizados para melhorar a performance das tarefas, e especialmente no caso do R essas escolhas tem um impacto gigantesco na performance do scripts. Assim, neste post vou...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">A linguagem R oferece ferramentas que podem ser usadas para visualização, modelagem e leitura de bancos de dados. Mas uma de suas características mais importantes é que é uma excelente ferramenta para preparação de dados. Naturalmente, como em outras linguagens, existem alguns truques que podem (e devem!) ser utilizados para melhorar a performance das tarefas, e especialmente no caso do R essas escolhas tem um impacto gigantesco na performance do scripts. Assim, neste post vou apresentar alguns dos truques mais importantes que devem ser utilizados no R e também vou apresentar um método muito importante na preparação que é o split-apply-recombine.</p>
<h2 style="text-align: justify;">1. Usando o apply, lappy, tapply</h2>
<p style="text-align: justify;">Algumas vezes os apply&#8217;s podem deixar o código mais rápido ou no mínimo eliminar alguns for&#8217;s deixando o código mais legível. O fato é que, pelo menos no R, é fortemente aconselhável <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://stackoverflow.com/questions/2908822/speed-up-the-loop-operation-in-r">evitar o uso do for</a></span>. Dessa forma, ao invés de usar o for é melhor executar as interações em matrizes, listas e data.frames usando a família apply. Um exemplo:</p>
<pre class="lang:r decode:true ">matriz &lt;- matrix(round(runif(9,1,10),0),nrow=3)
apply(matriz, 1, sum) ## sum by row
apply(matriz, 2, sum) ## sum by column</pre>
<p style="text-align: justify;">Particularmente neste exemplo você pode não ter um ganho de performance, mas o código pelo menos fica um pouco mais legível.</p>
<p style="text-align: justify;">Já que estamos falando de médias, o tapply é especialmente útil na tarefa de obter médias por grupo. A seguir um exemplo onde você consegue tirar uma média para grupos em uma única linha:</p>
<pre tabindex="0">mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3
Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3
Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4
Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4
Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4
Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1
Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1
Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1
Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4
Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1
Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4
Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6
Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8
Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2</pre>
<p>com</p>
<pre class="lang:r decode:true ">tapply(mtcars$hp, mtcars$cyl, mean)</pre>
<p>e você tem a média da potência por capacidade do cilindro. Esta função é extremamente útil em análise descritiva, principalmente quando você tem uma variável numérica que deve ser resumida em função de outra variável categórica. Entretanto algumas vezes temos listas e não vetores tal que é necessário fazer uso do lapply. Vamos gerar dados de exemplo:</p>
<pre class="lang:r decode:true ">lista &lt;- list(a=c('one', 'tow', 'three'), b=c(1,2,3), c=c(12, 'a'))</pre>
<p>Cada elemento na lista é um vetor. Digamos que você queira saber quantos elementos existem em cada vetor, mas sem usar um for! Vejamos:</p>
<pre class="lang:r decode:true ">lapply(lista, length) ## return a list
sapply(lista, length) ## coerce to a vector</pre>
<pre tabindex="0">$a
[1] 3

$b
[1] 3

$c
[1] 2</pre>
<pre tabindex="0">a b c 
3 3 2</pre>
<h2>2. Split, apply and recombine</h2>
<p style="text-align: justify;">Esta é uma técnica que simplesmente DEVE SER CONHECIDA. É o fundamento por detrás de pacotes como o <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://cran.r-project.org/web/packages/plyr/index.html">plyr</a></span> e <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://cran.r-project.org/web/packages/dplyr/index.html">dplyr</a></span> e é a mesma estratégia utilizada no <span style="color: #0000ff;"><a style="color: #0000ff;" href="https://pt.wikipedia.org/wiki/MapReduce">MapReduce</a></span>. Aqui vou usar somente funções do pacote base mas em outra oportunidade planejo apresentar as vantagens de fazer isso com dplyr. Voltando para o conjunto de dados mtcars, suponha que queiramos ajustar modelos de regressão da variável mpg (milha por galão) contra disp, agrupado por gears, e então comparar os coeficientes das regressões. Como fazemos isso?</p>
<p style="text-align: justify;"><a href="http://www.flaviobarros.net/wp-content/uploads/2013/10/mpg1.png"><img class="aligncenter size-full wp-image-323" src="http://www.flaviobarros.net/wp-content/uploads/2013/10/mpg1.png" alt="mpg" width="480" height="480" srcset="./../../../wp-content/uploads/2013/10/mpg1.png 480w, ./../../../wp-content/uploads/2013/10/mpg1-150x150.png 150w, ./../../../wp-content/uploads/2013/10/mpg1-300x300.png 300w, ./../../../wp-content/uploads/2013/10/mpg1-90x90.png 90w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<pre class="lang:r decode:true ">data &lt;- split(mtcars, mtcars$gear) ## split
fits &lt;- lapply(data, function(x) return(lm(x$mpg~x$disp)$coef)) ## apply
do.call(rbind, fits) ## recombine</pre>
<pre tabindex="0">  (Intercept)      x$disp
3    24.51557 -0.02577046
4    39.56753 -0.12221268
5    31.66095 -0.05077512</pre>
<p>Como você pode ver, em três passos está pronto: separa, aplica e agrega. É uma técnica poderosa que pode ser aplicada em diferentes contextos. Veja que o tapply usa esta estratégia internamente para calcular as médias por grupo: split no grupo, aplica a função média, junta as médias para apresentar no final.</p>
<p>Você conseguiria reproduzir o resultado do tapply com o split e o sapply? Poste nos comentários!</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2015/09/02/preparacao-de-dados-parte-1/feed/index.html</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Reconhecimento de dígitos escritos a mão &#8211; Parte 1</title>
		<link>./../../../2014/12/22/reconhecimento-de-digitos-escritos-mao-parte-1/index.html</link>
		<comments>./../../../2014/12/22/reconhecimento-de-digitos-escritos-mao-parte-1/index.html#comments</comments>
		<pubDate>Mon, 22 Dec 2014 18:58:00 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Aprendizado de Máquina]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[Preparação de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>
		<category><![CDATA[k-nn]]></category>
		<category><![CDATA[mnist]]></category>

		<guid isPermaLink="false">http://www.flaviobarros.net/?p=493</guid>
		<description><![CDATA[A tarefa de reconhecimento de dígitos escritos a mão foi um dos primeiro grandes sucessos dos métodos de aprendizado de máquina. Hoje em dia, a tarefa pode ser realizada por diversas bibliotecas especializadas com altíssima acurácia (&#62; 97% de acertos), tal que muitas vezes, apesar de utilizarmos indiretamente esses recursos em tablets e smartphones, em geral não sabemos exatamente como o método funciona. Pensando nisso, como já trabalhei com esse problema antes, vou demonstrar nesse post...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">A tarefa de reconhecimento de dígitos escritos a mão foi um dos primeiro <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://en.wikipedia.org/wiki/Handwriting_recognition">grandes sucessos</a></span> dos métodos de aprendizado de máquina. Hoje em dia, a tarefa pode ser realizada por diversas <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://opencv.org/">bibliotecas especializadas</a> </span>com altíssima acurácia (&gt; 97% de acertos), tal que muitas vezes, apesar de utilizarmos indiretamente esses recursos em tablets e smartphones, em geral não sabemos exatamente como o método funciona.</p>
<p style="text-align: justify;"><a href="./../../../wp-content/uploads/2014/12/3_032.BMP.png" rel="attachment wp-att-506"><img class="aligncenter size-full wp-image-506" src="./../../../wp-content/uploads/2014/12/3_032.BMP.png" alt="3_032.BMP" width="64" height="64" /></a></p>
<p style="text-align: justify;">Pensando nisso, como já trabalhei com esse problema antes, vou demonstrar nesse post como o processo funciona, as técnicas utilizadas e como implementar tudo na linguagem R. Para começar, vamos trabalhar com o problema de reconhecer se o dígito é 0,1,2,3,4,5,6,7,8,ou 9, isto é, um problema de classificação com 10 categorias.</p>
<p style="text-align: justify;">Vou tentar trabalhar aqui implementando toda a modelagem somente com as funções do pacote base e uns poucos pacotes extras com as funções e algoritmos necessários; em um próximo post, posso tentar utilizar outros pacotes para automatizar as diversas etapas da modelagem.</p>
<p style="text-align: justify;"><span id="more-493"></span></p>
<h3 style="text-align: justify;">1. LEITURA</h3>
<p style="text-align: justify;">Os dados do problema são imagens do tipo <a href="http://en.wikipedia.org/wiki/Netpbm_format"><span style="color: #0000ff;">PGM</span></a>, com 64 x 64 pixels por imagem, onde cada pixel tem valor 1 ou 0, indicando se o pixel é preto ou branco. Cada imagem tem um nome no formato X_ yyy.BMP.inv.pgm, onde o X representa o dígito desenhado na imagem. Os dados estão divididos em um conjunto de treino e um conjunto de teste e podem ser baixados nos seguintes links: <a href="./../../../wp-content/uploads/2016/03/teste.zip">teste</a>  e <a href="./../../../wp-content/uploads/2016/03/treino.zip">treino</a></p>
<p style="text-align: justify;">Assim a primeira parte do problema é efetuar a leitura dos dados. Para isso me utilizarei do pacote <span style="color: #0000ff;"><a style="color: #0000ff;" href="http://cran.r-project.org/web/packages/pixmap/index.html">pixmap</a></span> com o qual é possível ler e manipular imagens PGM. A seguir, o processo de leitura das imagens e a criação de um vetor com as respostas, isto é, com o número do dígito que está escrito na imagem.</p>
<pre class="lang:r decode:true">## Carregando o pacote pixmap
library(pixmap)

## Definindo o diretório com as imagens de treino
path_treino &lt;- '/sua/pasta/treino/'

## Define como diretório de trabalho
setwd(path_treino)

## Lê os nomes dos arquivos
files &lt;- dir()

## Retira as classes dos nomes dos arquivos
classes &lt;- as.factor(substring(files,first=1,last=1))

## Cria data.frame para armazenar dados de treino
treino &lt;- as.data.frame(matrix(rep(0,length(files)*64*64), nrow=length(files)))

## Efetua leitura dos dados de treino
for (i in 1:length(files)) {

  ## Lendo as imagens
  x &lt;- read.pnm(files[i])

  ## No slot 'grey' está a matriz de pixels que é retirada e vetorizada
  treino[i,] &lt;- as.vector(x@grey, mode='integer')
}

## Define como diretório de trabalho o local das imagens para teste
path_teste &lt;- '/sua/pasta/teste/'

## Diretório de trabalho
setwd(path_teste)

## Lê os nomes dos arquivos
files &lt;- dir()

## Classes
predic &lt;- as.factor(substring(files,first=1,last=1))

## Cria data.frame para armazenar conjunto de teste
teste &lt;- as.data.frame(matrix(rep(0,length(files)*64*64), nrow=length(files)))

## Leitura do conjunto de teste
for (i in 1:length(files)) {
  x &lt;- read.pnm(files[i])
  teste[i,] &lt;- as.vector(x@grey, mode='integer')
}</pre>
<p style="text-align: justify;">É importante observar que a matriz de pixels fica armazenada no slot @grey, e que após a leitura é transformada em um vetor, tal que o data.frame final fica com 64&#215;64 colunas e 1949 linhas (o total de imagens). O conjunto de  teste tem somente 50 imagens, logo o data.frame vai ficar com 64&#215;64 colunas e somente 50 linhas. Em suma, cada coluna é um píxel e cada linha uma das diferentes imagens.</p>
<h3> 2. MODELAGEM COM k-nn</h3>
<p style="text-align: justify;">Nessa etapa será realizado o aprendizado com o algoritmo k-nn (vizinhos mais próximos) sem nenhum tratamento dos dados. O algoritmo funciona atribuindo as classes às imagens, utilizando os valores conhecidos dos vizinhos mais próximos. Assim, digamos que k=3, o algoritmo busca as três imagens mais próximas, verifica qual é a classe majoritária dessas imagens e atribui essa classe à imagem sem label. É importante escolher um k ímpar para não ocorrer empates, por exemplo 2 vizinhos de uma classe e 2 de outra no caso de k=4.</p>
<pre class="lang:r decode:true">## Carrega pacote class com o k-nn
library(class)

## Utilizando o k-nn para previsão do dígitos nas imagens de teste
predito &lt;- knn(train=treino, test=teste, cl=classes, k=3, prob=T)

## Resultado
result &lt;- data.frame(cbind(predic, predito, acerto = predic==predito))

## Cálculo da taxa de acerto
sum(result$acerto)/nrow(result)

[1] 0.56</pre>
<p>E com um k=3 obtivemos uma taxa de acerto de somente 56%, muito aquém do que pode ser obtido. Assim, vamos rodar o algoritmo com diversos valores de k e verificar se conseguimos obter um resultado um pouco melhor.</p>
<pre class="lang:r decode:true">## Data.frame com todos os resultados
resultado &lt;- data.frame(k = rep(0,101), taxa=rep(0.00,101))

for (i in seq(from=1, to=101, by=2)) {
  
  ## Imprime o valor de k para controle
  print(i)
  
  ## Obtém os valores preditos para as imagens
  predito &lt;- knn(train=treino, test=teste, cl=classes, k=i, prob=T)
  
  ## Salva em um data.frame
  result &lt;- data.frame(cbind(predic, predito, acerto = predic==predito))
  
  ## Calcula a taxa de acerto e armazena no data.frame
  resultado[i,] &lt;- c(i,sum(result$acerto)/nrow(result))
}

## Elimina linhas com 0
resultado &lt;- subset(resultado, subset=resultado$taxa!=0)

## Plota o resultado para todos os k's
plot(resultado$taxa~resultado$k, main='Taxa de Acerto para o k-nn', xlab='Valores de K', ylab='Taxa de acerto')</pre>
<p>Obtendo o seguinte resultado:</p>
<p><a href="http://www.flaviobarros.net/wp-content/uploads/2014/12/valores_k.png"><img class="aligncenter size-full wp-image-502" src="http://www.flaviobarros.net/wp-content/uploads/2014/12/valores_k.png" alt="valores_k" width="480" height="480" srcset="./../../../wp-content/uploads/2014/12/valores_k.png 480w, ./../../../wp-content/uploads/2014/12/valores_k-150x150.png 150w, ./../../../wp-content/uploads/2014/12/valores_k-300x300.png 300w, ./../../../wp-content/uploads/2014/12/valores_k-90x90.png 90w, ./../../../wp-content/uploads/2014/12/valores_k-130x130.png 130w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<p style="text-align: justify;">Vejam que obtivemos algo em torno de 78% com k=1, mas que ainda é um resultado muito ruim perto do que pode ser alcançado. Também vale notar que aumentar o k não ajuda muito no fim das contas, mas é importante ficar atento pois um k muito pequeno pode levar ao superajustamento ou overfitting.</p>
<h3>CONCLUSÃO PARCIAL</h3>
<p style="text-align: justify;"> Ao que tudo indica a classificação de imagens funciona bem utilizando um algoritmo simples, sem nenhum tipo de tratamento. ENTRETANTO, é possível fazer muito melhor. Na Parte 2 vamos automatizar algumas tarefas com pacote <a href="http://caret.r-forge.r-project.org/"><span style="color: #0000ff;">caret</span></a> e também vamos explorar outros algoritmos melhores como o <a href="http://en.wikipedia.org/wiki/Support_vector_machine"><span style="color: #0000ff;">SVM</span></a> e o <a href="http://en.wikipedia.org/wiki/Random_forest"><span style="color: #0000ff;">RandomForest</span></a>.</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2014/12/22/reconhecimento-de-digitos-escritos-mao-parte-1/feed/index.html</wfw:commentRss>
		<slash:comments>3</slash:comments>
		</item>
		<item>
		<title>Data Preparation &#8211; Part I</title>
		<link>./../../../2013/10/31/data-preparation-tricks-part-i/index.html</link>
		<comments>./../../../2013/10/31/data-preparation-tricks-part-i/index.html#comments</comments>
		<pubDate>Thu, 31 Oct 2013 17:22:05 +0000</pubDate>
		<dc:creator><![CDATA[Flavio Barros]]></dc:creator>
				<category><![CDATA[Aprendizado de Máquina]]></category>
		<category><![CDATA[Data Mining]]></category>
		<category><![CDATA[English]]></category>
		<category><![CDATA[Estatística]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Mineração de Dados]]></category>
		<category><![CDATA[R e RStudio]]></category>
		<category><![CDATA[r-bloggers]]></category>

		<guid isPermaLink="false">http://www.flaviobarros.net/?p=281</guid>
		<description><![CDATA[The R language provides tools for modeling and visualization, but is still an excellent tool for handling/preparing data. As C++ or python, there is some tricks that bring performance, make the code clean or both, but especially with R these choices can have a huge impact on performance and the &#8220;size&#8221; of your code. A seasoned R user can manage this effectively, but this can be a headache to a new user. SO, in this...]]></description>
				<content:encoded><![CDATA[<p style="text-align: justify;">The R language provides tools for modeling and visualization, but is still an excellent tool for handling/preparing data. As C++ or python, there is some tricks that bring performance, make the code clean or both, but especially with R these choices can have a huge impact on performance and the &#8220;size&#8221; of your code. A seasoned R user can manage this effectively, but this can be a headache to a new user. SO, in this series of posts i will present some data preparation techniques that anyone should know about, at least the ones i know!</p>
<h2 style="text-align: justify;">1. Using apply, lappy, tapply</h2>
<p style="text-align: justify;">Sometimes the apply&#8217;s can make your code faster, sometimes just cleaner. BUT the fact is that, at least in R, is recommended <span style="color: #0000ff;"><a href="http://stackoverflow.com/questions/2908822/speed-up-the-loop-operation-in-r"><span style="color: #0000ff;">avoid for loops</span></a></span>. So, instead of using loops, you can iterate over matrixes, lists and vectors using these functions. As an example see this code:</p>
<pre class="lang:r decode:true ">matriz &lt;- matrix(round(runif(9,1,10),0),nrow=3)
apply(matriz, 1, sum) ## sum by row
apply(matriz, 2, sum) ## sum by column</pre>
<p>Particularly in this example there is no gain on performance, but you get a cleaner code.</p>
<p>Talking about means, sometimes tapply can be very usefull in this regard. Let&#8217;s say you want to get means by group, you can have this with one line too. For example, considering the mtcars dataset:</p>
<pre tabindex="0">mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3
Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3
Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4
Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4
Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4
Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1
Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1
Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1
Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4
Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1
Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4
Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6
Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8
Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2</pre>
<p>so</p>
<pre class="lang:r decode:true ">tapply(mtcars$hp, mtcars$cyl, mean)</pre>
<p>and you can have the mean power by cylinder capacity. This function is very usefull on descriptive analysis. BUT sometimes you have lists, not vectors. In this case just use lappy or sapply (simplify the output). Let&#8217;s generate some data:</p>
<pre class="lang:r decode:true ">lista &lt;- list(a=c('one', 'tow', 'three'), b=c(1,2,3), c=c(12, 'a'))</pre>
<p>Each element of this list is a vector. Let&#8217;s say you want to know how many elements there is in each vector:</p>
<pre class="lang:r decode:true ">lapply(lista, length) ## return a list
sapply(lista, length) ## coerce to a vector</pre>
<p>&nbsp;</p>
<pre tabindex="0">$a
[1] 3

$b
[1] 3

$c
[1] 2</pre>
<pre tabindex="0">a b c 
3 3 2</pre>
<h2>2. Split, apply and recombine</h2>
<p style="text-align: justify;">This technique you must know. Basically we split the data, apply a function and combine the results. There is a <span style="color: #0000ff;"><a href="http://cran.r-project.org/web/packages/plyr/index.html"><span style="color: #0000ff;">package</span></a></span> created with this in mind. But we will use just base R functions: split, *apply and cbind() ou rbind() when needed. Looking again at mtcars dataset, let&#8217;s say we want fit a model of mpg against disp, grouped by gears,  and compare the regression coefficients.</p>
<p style="text-align: justify;"><a href="http://www.flaviobarros.net/wp-content/uploads/2013/10/mpg1.png"><img class="aligncenter size-full wp-image-323" src="http://www.flaviobarros.net/wp-content/uploads/2013/10/mpg1.png" alt="mpg" width="480" height="480" srcset="./../../../wp-content/uploads/2013/10/mpg1.png 480w, ./../../../wp-content/uploads/2013/10/mpg1-150x150.png 150w, ./../../../wp-content/uploads/2013/10/mpg1-300x300.png 300w, ./../../../wp-content/uploads/2013/10/mpg1-90x90.png 90w" sizes="(max-width: 480px) 100vw, 480px" /></a></p>
<pre class="lang:r decode:true ">data &lt;- split(mtcars, mtcars$gear) ## split
fits &lt;- lapply(data, function(x) return(lm(x$mpg~x$disp)$coef)) ## apply
do.call(rbind, fits) ## recombine</pre>
<p>&nbsp;</p>
<pre tabindex="0">  (Intercept)      x$disp
3    24.51557 -0.02577046
4    39.56753 -0.12221268
5    31.66095 -0.05077512</pre>
<p>This technique is powerfull. You can use at different contexts.</p>
<p>Next part i will talk about some tricks with dates.</p>
]]></content:encoded>
			<wfw:commentRss>./../../../2013/10/31/data-preparation-tricks-part-i/feed/index.html</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
	</channel>
</rss>
